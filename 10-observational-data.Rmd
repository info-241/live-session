# Causality from Observational Data

```{r load unit10 packages, echo=FALSE, message=FALSE}
library(data.table)

library(sandwich)
library(lmtest)

library(ggplot2)
library(patchwork)

theme_set(theme_minimal())
berkeley_blue   <- '#003262'
california_gold <- '#FDB515'
```

![punkin belly](./images/punkin.jpeg)

What happens if we cannot run an experiment? Perhaps we don't have the budget or time, perhaps the context is too fraught to conduct an experiment. Should we walk away and learn nothing?

## Learning Objectives

At the end of this weeks *extensive* content, students should be able to

1.  **Describe** a series of techniques that have been proposed to estimate causal effects even when a randomized experiment has not been conducted;
2.  **Evaluate** whether a particular technique matches with the data generating context;
3.  **Analyze** whether an observational data technique is likely to identify a treatment effect; and,
4.  **Communicate** the risks and limitations that are brought about when using observational data to make causal claims.

## The Experimental Ideal

If you've been through this once, you've been through it one-hundred times this semester, but it might be worth re-stating what we get out of conducting a randomized experiment. Consider this a sage smudging at the beginning of a spooky week.

> Why do we conduct experiments? What guarantees exist as a result of a well-run experiment?

## A Continuum of Plausibility

As we are talking today, consider the fully-randomized, full-compliance, full-reporting, high-powered field experiment to be the high-water mark of credibility. Under such a scenario, we can think of any analysis that we undertake as producing a highly-credible, highly-reliable estimate of a treatment effect.

Through our discussion this week, we hope to name where we think other techniques and data generating processes fall relative to this high-water mark. Some, as we will see, might actually produce estimates that are *very nearly* as credible as the experimental ideal. Others are ghastly in their performance.

However, as data scientists who *have to get work done* we need to be able to produce the best possible statement about a treatment effect, and if we have any misgivings about those statements, be able to provide a clear statement about the risk that is attendant to using them.

## Natural Experiments

Natural experiments are experiments that have been conducted by someone *other* than the researcher. If you remember back to Problem Set 1, consider the case of the early childhood education that is provided by the state. When the state *chose* who to provide education to based on need, this was clearly not an experiment because it isn't possible to fully understand the selection criteria used by the state, and so it is not possible to make a strong statement that any estimate produced from a two-group estimator wouldn't be possibly subject to confounding.

But, what about the case where the state *randomly assigned* some kids to get the treatment? Is there any reason that we should discount this simply because it wasn't us to do the assigning? What hubris!

### Questions to consider

-   What are the hallmarks of a natural experiment?
-   How would you propose to structure your search for natural experiments?
-   How will you know when you've actually *found* something that is a natural experiment?

### Breakout activity

-   What are the things in the past year of your lives that have seemed to *arrive at random*? How would you know if they actually **are** at random?
-   After the members of the team have spent a few moments thinking about things that might be random, ask yourselves, "What might we be able to learn downstream from this experiment?" What is the most plausible thing that you might learn? What is the longest, most extreme possibility that you might learn?

### How does one analyze a natural experiment?

If a natural experiment is just a randomization conducted by someone else -- is there anything different that we need to do in order to analyze it? Why or why not?

We talk, with some specificity this week, about estimating using two-stage least squares regression. What is this technique, what does it promise to us, and how does it work?

Consider simulated data that is created in the following way:

-   `ability`, `family_income`, and `lottery` winning to get into a "magnet" school are all random
-   However, suppose that `schooling` which is the indicator that someone actually got schooling at a magnet school is correlated with ability, with family income, and with lottery.

What would be the consequence of estimating an eventual outcome, using a naive regression?

```{r make iv data, echo=FALSE}
set.seed(2)

d <- data.table(id = 1:1000)

d[ , ':='(
  ability       = rnorm(.N, mean=10, sd=1), 
  family_income = rnorm(.N, mean=20, sd=2), 
  lottery       = rnorm(.N, mean=10, sd=1) > 10
  )]

d[ , schooling := 1.0 * lottery         + rnorm(.N, mean=0, sd=1)]
d[ , Y         := ability + family_income + 2 * schooling   + rnorm(.N, mean=0, sd=1)]

model_wrong <- d[ , lm(Y ~ schooling)]
coeftest(model_wrong, vcov. = vcovHC)
```

-   How close, or far from the truth is this estimate? How sure are you that this is different from zero?
-   What relationship would you have to change in this data generating process in order to flip the bias of the estimate from estimating a value that is higher than the truth, to estimate a value that is lower than the truth?

## Can we fix this estimate?

The promise of two stage-least squares is that it produces unbiased estimates so long as we're able to find something that is random.

```{r estimate two stage regression}

first_stage <- d[ , lm(schooling ~ lottery)] 
d[ , schooling_hat:= predict(first_stage)] 

second_stage <- d[ , lm(Y ~ schooling_hat)]
coeftest(second_stage, vcov. = vcovHC)
```

-   How or why does this work?

-   How does simply making predictions from the first stage regression generate eventual estimates that are unbaised?

    -   Consider looking at the residuals from the first stage regression

```{r}
d[ , residuals := resid(first_stage)]

ability_by_lottery <- ggplot(d) + 
  aes(x=ability, fill = as.factor(lottery)) + 
  geom_density(alpha=0.5) + 
  labs(title='Ability by lottery')

residuals_lottery <- ggplot(d) + 
  aes(x=residuals, fill=as.factor(lottery)) + 
  geom_density(alpha=0.5) + 
  labs(title = 'Residuals by lottery')

residuals_ability <- ggplot(d) + 
  aes(x=residuals, fill=as.factor(ability>10)) + 
  geom_density(alpha=0.5) + 
  labs(title = 'Residuals by ability')

ability_by_lottery / residuals_lottery / residuals_ability
```

Another way to think about this is in terms of how the predicted values are associated different features. Specifically, consider: Are the predicted values associated with having won the lottery? Are the predicted values associated with ability?

```{r}

predicted_lottery <- ggplot(d) + 
  aes(x=as.factor(lottery), y=schooling_hat) + 
  geom_jitter() + 
  labs(title='Predicted Values and Lottery Winning') 

predicted_ability <- ggplot(d) + 
  aes(x=ability, y=schooling_hat, color = as.factor(lottery)) + 
  geom_jitter() + 
  labs(title='Predicted Values and Ability')

predicted_lottery / predicted_ability
```

## Regression Discontinuity

Regression discontinuity is a *really* clever idea, that when the data presents itself and the analysis is done correctly, provides a *very* compelling argument for having captured a causal effect.

The key insight in the case of regression discontinuity is that we might *not* need something that is actually random in order to produce a credible treatment effect. All we need is treatment assignment mechanism that is not correlated with potential outcomes. And, the argument for regression discontinuity is that if you make a comparison set similar enough along a scoring variable, then it would be very hard for people on one-side or the other-side of an arbitrary point in the scoring variable to be different.

### Why do RDD designs "work"?

-   What part of the RDD is producing an unbiased causal estimate?

-   Why is this part of he design/data generating process able to produce this unbiased causal estimate?

-   What is a "forcing" variable?

-   How do I identify *where* the cut-point in the forcing variable is located?

### Just how common are opportunities for RDD

Here's a controversial point of view: Everything that we do as data scientists is to make low-dimensional representations of higher dimensional space. Let's have a jam-session where the class and instructors take turns naming places where a RDD could be run. We'll start with:

-   Revolving line of credit -- credit scoring models bring in disparate streams of information, produce a low-dimensional 0-800 (or something like that...) rating and provide revolving lines of credit to different parts of the distribution.

-   Now you...

### Working with Regression Discontinuity Designs

Let's look at see what is happening when we're working with RDD designs. To start, let's build some data. Read through each of the lines below, and note what is happening with the data being created. (*Notice that we are chaining together the data.table after we create `y0` and again after we create `y1`.*)

```{r make RDD data}

N <- 1000

d <- data.table(id=1:N)
d[ , ':='(
  tau     = rnorm(.N, mean=0, sd=2), 
  running = runif(.N, min=-2, max=2), 
  y0      = runif(.N, min=-1, max=1)) ][ , 
  y1     := y0 + tau ][ , 
  Y      := ifelse(running > 0, y1, y0)]
```

With the data created, let's quickly look at what we are working with. Does the following plot seem to capture the idea of a treatment effect?

-   If so, why?

-   If not, why not and how would you propose to change the plot?

```{r first rdd plot}

ggplot(d) + 
  aes(x=running, y=Y) + 
  geom_point() + 
  stat_smooth(method = 'lm', se=FALSE)
```

```{r second rdd plot}
d[ , plot(x=running, y=Y, pch = 19, col=berkeley_blue)]
  d[running < 0, lines(lowess(running,Y), lwd=10, col=california_gold)]
  d[running > 0, lines(lowess(running,Y), lwd=10, col=california_gold)]
```

This has been *very* fortunate data. There is no trend across the running variable, and things seem mostly linear on both sides. Naturally, the real world is not so tidy.

### More realistic data

```{r slightly more realistic data}

d <- data.table(id=1:N)
d[ , ':='(
  running = runif(n=.N, min=0, max=10), 
  cov1    = rnorm(n=.N)) ][ ,
  Y := running * 0.1 - 0.2 * cov1 + 1 * I(running > 5) + rnorm(n=.N)]

d[ , plot(x=running, y=Y, col=berkeley_blue, pch=19)]
```

-   Is it clear if there is, or is not an effect in this data simply by looking at it?

-   What if you put a smoother through the data?

```{r}
d[ , plot(x=running, y=Y, col=berkeley_blue, pch=19)]
  d[ , lines(lowess(running, Y), col=california_gold, lwd=10)]
```

-   What if you break that smoother at the policy point?

```{r}
  d[running < 5.2 & running > 4.8 , plot(x=running, y=Y, col=berkeley_blue, pch=19)]
    d[running < 5 , lines(lowess(running, Y), col=california_gold, lwd=10)]
    d[running > 5 , lines(lowess(running, Y), col=california_gold, lwd=10)]  


```

### What about even more challenging data?

```{r}
d <- data.frame(running = runif(1000, min = 0, max = 10), 
                cov1    = rnorm(1000))
d$y <- d$running * 0.1 - .2 * d$cov1 + 1 * I(d$running > 5) +
    .4 * d$running * I(d$running > 5) + rnorm(1000)

plot(x = d$running, d$y, pch = 19, col = rgb(0,1,0, .4))
lines(lowess(d$running[d$running < 5], d$y[d$running < 5]))
lines(lowess(d$running[d$running > 5], d$y[d$running > 5]))
```

-   What model would you fit against this data?

```{r}

```
