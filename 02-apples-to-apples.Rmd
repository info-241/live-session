# Apples to Apples

```{r}
library(data.table)
library(ggplot2)
```


## Learning Objectives 

At the conclusion of this week's live session, student will be able to:

1. *Describe*, using the technical language of potential outcomes, what it means for an input to *cause* an output. 
2. *Describe* the fundamental problem of causal inference. 
3. *Apply* iid sampling as a method of producing an unbiased, consistent estimator of a population. 
4. *Prove* that the average treatment effect estimator produces an unbiased, consistent estimator for the average treatment effect. 

## Revisiting Ideas of Science 

Questions about epistemology are a *classic* question, and one that is particularly relevant not only at the School of Information where we have faculty and student whose work ranges from fields as diverse as computer science, psychology, sociology, law, and education -- "**What does it mean to *know* something?**" We'll note that this question is not only an academic question, because in our workplaces we need to know how to take the best course of action. In this course, we like to think of *Science* as a method of coming to know something. 

Think back to the reading and discussion from last week: For Feynman, what does it mean to be "Doing science?" Would Feynman say that data science, as we are practicing it, is a "science"? Would Feynman say that 205, or computer science is a science? Would Feynman say that 203 is a science? What about 251, 255, or 266? 

For Lakatos, what does it mean for something to be a part of a science? What does it mean for something to be a part of a psuedo-science? Is it as simple a view as Feynman espouses? Does the work that we conduct across the coursework in this program produce scientific knowledge as Lakatos see it? 

What do you think produces knowledge? Can a single conversation produce knowledge? Can a non-experimental study produce knowledge about a causal effect? Can an experiment fail to produce knowledge? If an experiment fails to reject some null hypothesis, does that mean that it has not produced any knowlege? 

## This Causes That

What does it mean for an action to cause an outcome? Don't worry about conducting the experiment, or any measurement concerns at this point, just engage with the concepts. 

### Damn fine coffee

<iframe width="560" height="315" src="https://www.youtube.com/embed/Uvs7pmISe8I" title="Damn Fine Coffee" frameborder="0"></iframe>

Suppose that you're getting ready for class, and you want to make sure that you're at your best. So, you drink a cup of water, eat a small snack, and brew a small pot of coffee for while you're in class. 

> Why do you do this? 

Presumably, you're doing this because you like each of these things, but also because you're interested in these things causing you to have a better class. If you framed this as as causal question, you might ask: 

> If I drink a cup of coffee before class, will it cause me to be more alert? 

What does it mean for coffee to cause alertness? 

- Does coffee cause everyone to become more alert? 
- Does coffee have to affect everyone equally in order for you to say it causes alertness? 
- Could coffee have no effect for some people, and you would still say it causes alertness? 
  
### Meditation for focus 

Suppose that you're getting ready for class, and you want ensure that you're at your best. So, you find a quiet place, and set your mind at ease with whatever form of meditation you think might be helpful. 

> If I meditate before class, will it cause me to be more focused? 

What does it mean for meditation to cause focus? 

- Does meditation cause everyone to become more focused? 
- Does meditation have to affect everyone equally? 
- Some people are frustrated by not being able to quiet their thoughts, and actually find meditation frustrating. Can this be true, and still believe that meditation causes focus? 

### Selling coffee and meditation

Suppose that you're an enterprising soul, and you want to sell a book about brewing coffee as a meditation. You reason that there must be a niche for this approach. To get the word out, you place a few flyers with tear off phone-numbers at the local yoga studios and tech incubators (good intuition to find those MIDS students). 

> If shown a flyer for coffee-meditation, will it cause someone to take my training? 

What does it mean for for flyers to cause people to sign-up for the training?

- Does the flyer cause everyone to take the training? 
- Does the flyer affect everyone equally? 

### Reflecting on Causes

Does anything unify questions of causes? 

When you think about *{this}* causing *{that}*, do you think about it at a population level, a smaller group level, or at the individual level? 

## Reading Discussion: The Power of Experiments 

*The Power of Experiments* starts the discussion of experimentation in the workplace with what is, for the course instructors, a uniquely pedestrian example, increasing contributions to taxes. In particular, Her Majesty's Revenue and Customs sends different versions of a letter to British taxpayers, and observes that different language leads to different amounts of taxes being paid. 

### Chapter One: The Power of Experiments

1. Is it *actually* a "big-deal" to increase tax compliance by 2 percentage points? 
2. On page five, the book identifies five "one-liners" that HMRC chose to send to taxpayers: 
  1. *Nine out of ten people pay their tax on time."* 
  2. *Nine out of ten people in the UK pay their tax on time.*
  3. *Nine out of ten people in the UK pay their tax on time. You are currently in the very small minority of people who have not paid us yet."*
  4. *Paying tax means we all gain from vital public services like the NHS, roads and schools.*
  5. *Not paying tax means we all lose out on vital public services like the NHS, roads, and schools.* 
3. Which of these sentences would be the most effective at getting you to pay your taxes? Which do you think will be most effective, overall, at generating tax compliance? Why? How willing are you to make a million pound bet that you're correct? 
4. Some of your instructors are vegetarians. None of them, however, has previously made an argument for why everyone should be vegetarian based on the example of Daniel and his study of diet and divine intervention. What about the study that Daniel conducted produces evidence that you think is useful for evaluating diet? What are the limitations that you see in this study? The book lists several, but there are other issues, along the lines of  the *exclusion restriction* that *Field Experiments* identifies. 
5. In order for Pasteur to be declared the winner of the vaccine argument, the observers said that every control group sheep had to die and every treatment group (i.e. vaccine-receiving) sheep had to live. Is this a fair burden of proof? Do the frequentest tests that we developed in 203 and are going to use here in 241 set a higher or lower bar than Pasteur faced? What are the merits of a relatively higher or lower bar? 

### Chapter Two: The Rise of Experimetnts in Psychology and Economics

Freud is noted as being specifically *against* experimentation. But, *PoE* then goes on to write, "[Freud's] big ideas inspired entire fields of psychological research. Including the notion that unconscious processes shape our judgement and behavior, psychological disorders are rooted in the mind rather than the body; and that sexual urges and behavior are worthy of study" (p. 19). Some of the theories that Freud promulgated were found to have evidence that was consistent with the theory; some of these theories could not produce evidence to support the theory; and many were outright contradicted by the evidence.

1. Is there value in being an "idea person"? How would you ever know if your ideas were actually right if you're unwilling to evaluate them? 
2. What, if any, are the limitations of experimenting without any "big ideas" to ground your experiments? 

Behaviorists (Skinner is the leading behaviorist) make a compelling argument: "One cannot directly observe what is happening in the mind of a person." A classic implication of this argument for behaviorists is that only that which is empirically observable is reasoned about. "Why does the rat avoid getting shocked? Does it really matter?" "Why does the child want a cookie? Does it really matter why?" 

1. Is this position reasonable for you to take as you navigate your own life? If you spoke with a therapist or a coach and said, "I've been feeling stressed over the past several weeks," would be satisfied with a *mindful* answer like, "Well, let's acknowledge those feelings and hold them for a moment" or would you want to reason further about why you feel stressed? What are the types of things in people's heads that you think we can profitably reason about; what are the types of things in people's heads that we cannot reason about? Is there something that is common to those that we can or cannot work with? 

The experiments of Milgram and Zimbardo are widely identified as the reason that human-subjects review boards no exist. These review boards serve as an external review that keeps researchers from inflicting harm to individuals that is not outweighed by societal or scientific benefits. 

1. What did Milgram and Zimbardo do to their subjects?
2. By talking continuing to talk about these experiments nearly fifty years after they were conducted -- even if we are talking about them negatively -- are we adding to the fame of these researchers? (For those interested in inside baseball, Zimbardo was the president of the American Psychology Association in 2002, and was awarded a lifetime achievement award from his discipline.) How should we learn and react to work that shouldn't have been conducted in the first place? 

Kahneman and Tversky propose that individuals think about expected values differently depending on whether they are thinking in the domain of gains or the domain of losses. They come to this theory through the, now cringe-worthy, *Asian Disease Problem*: 

In the positive frame, they ask the question: 

> Imagine that the US is preparing for the outbreak of an unusual Asian disease that is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. 
> 
> - If **Program A** is adopted, 200 people will be saved. 
> - If **Program B** is adopted, with a 1/3 probability 600 will be saved and with a 2/3 probability nobody will be saved.
> 

The authors also present a countervailing pair of scenarios framed in terms of losses

> - If **Program C** is adopted, 400 people will die. 
> - If **Program D** is adopted, there is a one-third probability that no one will die, and a 2/3 probability that everyone will die. 

Clearly, all these programs have the same expected number of deaths; but, people can disagree about which of these is the program that we should pursue. Just ask as a poll in the class; and, ask people to justify their beliefs. 

### The Rise of Behavioral Experiments in Policymaking 

*PoE* points out that experiments abound in policy making. Part of this stems from a truthful ignorance of the optimal policy to pursue. Another part of this stems from the ability of policy makers to make decisions by fiat that affect a large number of people. 

1. Does this justification for experiments align with your current understeanding of the landscape in human-facing data science? 

In a section titled **The nuance behind behavioral insights** the authors state a series of three caveats: 

> 1. *Context matters*
> 2. *Design choices matter* 
> 3. *Unintended consequences abound* 

1. What do they mean when they raise the three points?
2. We are going to ask you to justify conducting experiments by staking out an extreme point of view, and asking you to convince us that this point of view is so extreme that it cannot be justified. *"Writing that context matters, design choices matter, and unanticipated consequences abound is little more than writing that experiments cannot produce any more useful insights than the theories of Freud. As a result, there is little reason to conduct any experiments because what we learn will be highly contextualized, affected by very small implementation choices, and may generate as many (or more) negative outcomes as positive outcomes."* 

## Potential Outcomes

Potential outcomes are a system of reasoning, and a corresponding notation, that allow us to talk about observable and un-observable characteristics of the world. 

> What is your position on *ontology*? What does it mean for something to exist? 

- Does *Field Experiments*, as a textbook, exist? 
- Do Don Green and Alan Gerber, the authors of the textbook that we're reading, exist? 
- Does David Reiley, the slower-talking Davids in the async, exist? 
- Do I, your section, instructor, exist (or am I a deep fake in this room with you)?
- Can a concept exist, even if you can't hold it? Even if you haven't seen it? 

### Defining Potential Outcomes 

For each of the following sets of notation: (1) Read the notation aloud, not as "Y sub i zero", but instead as "The potential outcome to control ...". 

- $Y_{i}(0)$: 
- $Y_{i}(1)$:  
- $E[Y_{i}(0)]$: 
- $E[Y_{i}(1)]$: 
- $E[Y_{i}(0)|D_{i}=0]$:
- $E[Y_{i}(1)|D_{i}=1]$:
- $E[Y_{i}(0)|D_{i}=1]$:
- $E[Y_{i}(1)|D_{i}=0]$:

- Which of these concepts that you have just read aloud exist? 
- Can a concept exist, even if you can't hold it? Even if you can't see it? 

## Using Independence 

Suppose that you have a random variable that is defined as the function, 

$$
Y = 
  \begin{cases}
    \frac{1}{10} & ,0 \leq y \leq 10 \\ 
    0 & \text{, otherwise}
  \end{cases}
$$

- What is the expected value of this function? 

$$
\begin{aligned}
  E[Y]  &= \int_{0}^{10} y \cdot f_{y}(y) \ dy                       \\ 
        &= \int_{0}^{10} y \cdot \frac{1}{10} \ dy                   \\ 
        &= \frac{1}{10}\int_{0}^{10} y \ dy                          \\ 
        &= \left.\frac{1}{10} \cdot \frac{1}{2}  y^2\right|_{0}^{10} \\ 
        &= \left.\frac{1}{20}  y^{2} \right|_{0}^{10}                \\ 
        &= \frac{1}{20} \cdot \left[(100) - (0) \right]              \\ 
        &= \frac{1}{20} \cdot 100                                    \\
        &= \mathbf{5}
\end{aligned}
$$

- Why is the expected value a good characterization of a random variable?

- If you wanted to write down an estimator to produce a summary statistic for $Y$ given a sample of data, what properties do the following estimators possess: 

- $\hat{\theta}_{1} = y_{1}$
- $\hat{\theta}_{2} = \frac{1}{2} \displaystyle\sum_{i=1}^{2} y_{i}$
- $\hat{\theta}_{3} = \frac{1}{n-1} \displaystyle\sum_{i=1}^{N} y_{i}$
- $\hat{\theta}_{4} = \frac{1}{n} \displaystyle\sum_{i=1}^{N} y_{i}$

```{r make population function} 
conduct_sample <- function(size) { 
  runif(n=size, min=0, max=10)
}
```

```{r write estimators}
theta_1 <- function(data) { 
  # take the first element
  
}

theta_2 <- function(data) { 
  # sum the first two elements and divide by two
  
}

theta_3 <- function(data) { 
  # sum the sample, and divide by 1 less than the sample size
  
}

theta_4 <- function(data) { 
  # sum the sample, and divide by the sample size 
  # honestly, just use the mean call. 
  # clearly, this is a silly function to write, since you're just 
  # providing an alias, without modification, to an existing function. 
  
  mean(data)
  
}
```

```{r}
theta_4(conduct_sample(size=100))
```

- Just to put a fine point on it: **What estimator properties does the sample average provide, and why are these desirable?" 

## Use Randomization to Produce Independence 

How can we use the independence that is induced by "random **assignment** to treatment" combined with the sample average estimator to produce an estimate of an otherwise very difficult concept to measure? 

## Theoretical Justification

Before we show that this very simple ATE estimator work against a sample of data, it is worth reasoning about whether we can guarantee that it works in a general case. If we can show that it works in a general case, then any specific case inherits that guarantee. However, if we can only reason thorugh the existence of a single examlpe, it is not a sufficient argument to compell us to believe that it must hold for all cases. 

Here's an example, "Behold! I see a black sheep! Therefore all sheep are black." This doesn't make sense, and it is not a logically sound argument. However, if you say, "All sheep say, 'Baaah!' This is a black sheep, so it must say 'Baah!'" is a logically sound arugment, so long as the antecedent is, in fact true. When we're proving something, we're proving that the antecedent to this statement is generally true. For anyone who took a symbolic logic course in, this method of argument might be marked down as $\forall X \implies \exists X$, whereas $\exists Y \not\Rightarrow \forall Y$. 

- What is $\tau_{David}$? 
- What is $\tau_{i}$? 
- Is there any reason to believe that $\tau_{David\ Reiley} = \tau_{David\ Broockman}$? 
- Is there any reason to believe that $\tau_{i} = \tau_{j}$, where $j \neq i$? 
- What is the fundamental problem of causal inference?


The proof for this argument is also made in *Field Experiments*, on or about page 30 of the text. However, in our view, the authors don't give enough room to fully develop this proof, and so we skipped right past it the first time that we read the chapter. 

Begin our proof with the statmenet for what a treamtent effect is, $\tau_{i}$. 

$$
  \begin{aligned} 
    \tau_{i}    &= Y_{i}(1) - Y_{i}(0)    & Definition   \\ 
    ATE         &= E[\tau]                & Definition   \\
                &= E[Y(1) - Y(0)]         & Substitution \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
                &=                                       \\
  \end{aligned}
$$

## Simulation Example

Now, let's work through an example that shows this works not only in the math, but also in the realized, i.e. sampled, world. 

To begin with, lets work with a *very* simple sample that has 100 observations, potential outcomes to control are uniformly distributed between `0` and `1` and every single unit has a potential outcome to treatment that is `0.25` units larger than their potential outcomes to control.  

```{r create data}
make_simple_data <- function(size=100) { 
  require(data.table) 
  
  d <- data.table(id = 1:100)  
  
  d[ , y0 := runif(.N, min = 0, max = 1)]
  d[ , y1 := y0 + .25]
  
  return(d)
  }

d <- make_simple_data(size=100)
```

In this world, we've taken a sample of `100` individuals, and at this point, each of those individuals that we've sampled has both a potential outcome to control **and also** a potential outcome to treatment. We haven't talked at all about measurement yet; we're just asserting that both of these potential outcomes exist for each person. 

Essentially, this stage of creating the sample is the same as bringing people in the door to your experiment. If you were running this in the laboratory, you'd literally think of this as sitting your subjects down at their chairs, getting ready to begin their task. 

> Is randomly sampling people to be a part of your experiment sufficient to ensure that your experiment produces an unbiaed, consistent estimate of the true treamtent effect? 

Suppose that for each unit, you then toss a coin, placing the subject either into treatment or control based on the result of that coin flip. 

- Does this coin flip ensure that you have the same number of units in treatment as control? Does this matter to you? Why or why not? 
- Are there other ways that you could assign individuals to treatment an control, rather than through a simple-randomization process? 
- What are the relative merits or limitations of each of the methods?
- Are some of these methods *more random* than others? Or, are all things that are random equal in their randomness? 

### Assign to Treatment and Control
```{r assign units to treatment and control at random}
d[ , experimental_assignment := sample(0:1, size = .N, replace = TRUE)]
```

As as comparison, suppose that instead of randomly assigning individuals into treatment and control we allowed individuals to select into treatment and control. And suppose that people with the lowest potential outcomes to control opt to take the treatment. You might think of this as being something like, "The people who are the most tired are the most likely to drink a cup of coffee before they start class," if an example helps you ground this. 

Specifically, suppose that any unit that has a potential outcome lower that `0.33` opts to take the treatment. 

```{r allow observational selection into treatment}
d[ , observational_selection := ifelse(y0 < .33, 1, 0)]
```

These represent two different ways that you might conduct your research, each time with the same subject pool. Of course, in reality you probably would not be able to run these two studies at the same time, but since this is a simulation, we can stretch the confines of reality just a little bit. 

```{r}
first_plot <- ggplot(data=d) + 
  geom_point(aes(x = id, y = y0), color = 'steelblue') + 
  geom_point(aes(x = id, y = y1), color = 'darkorange')
first_plot  
```

What's actually happening in this? It might be more clear if we add arrows to this plot to show. 

```{r show the movement}
first_plot +
  geom_segment(
    aes(x = id, xend = id, y = y0, yend = y1), 
    arrow = arrow(ends = 'last', length = unit(0.05, "inches"), type = 'closed'), 
    color = 'grey70'
    )
```

Even though these potential outcomes exist for all the units, is it possible to actually see them for all the units? How do we go about showing, and then measuring the potential outcomes to control for a set of units? How about the potential outcomes to treatment? 

```{r}
second_plot <- ggplot(data = d) + 
  geom_point(aes(x = id, y = y0, size = 1 - experimental_assignment), color = 'steelblue') + 
  geom_point(aes(x = id, y = y1, size = experimental_assignment - 1), color = 'darkorange')

second_plot
```

What are the averages of these samples that have been assigned to treatment? 

```{r}
third_plot <- second_plot + 
  geom_hline(yintercept = mean(d[experimental_assignment==0, y0]), color = 'steelblue', linetype = 2) + 
  geom_hline(yintercept = mean(d[experimental_assignment==1, y1]), color = 'darkorange', linetype = 2)
third_plot
```

Even though we aren't able to see it, can we reason about what the sample average would be if we could see both of an individual's potential outcome to treatment and control? 

- Is there a guarantee that the sample should be the same as the feasible realization? 
- Should they be close? What property from 203 provides this guarantee? 

```{r}
third_plot + 
  geom_hline(yintercept = mean(d[, y0]), color = 'steelblue', linetype = 1) + 
  geom_hline(yintercept = mean(d[, y1]), color = 'darkorange', linetype = 1)
```

Put it all together, what has this little demo shown? 

### What if there is selection?

What if, rather than being assigned to treatment and control, instead individuals had been able to opt into treatment and control? 

Produce only the last plot, but this time for the observational, or selected data. 

```{r}
selection_plot <- ggplot(d) + 
  geom_point(aes(x = id, y = y0, size = 1 - observational_selection), color = 'steelblue') + 
  geom_point(aes(x = id, y = y1, size = observational_selection - 1), color = 'darkorange') + 
  geom_hline(yintercept = d[observational_selection == 0, mean(y0)], color = 'steelblue', linetype = 2) + 
  geom_hline(yintercept = d[observational_selection == 1, mean(y1)], color = 'darkorange', linetype = 2) + 
  geom_hline(yintercept = mean(d[, y0]), color = 'steelblue', linetype = 1) + 
  geom_hline(yintercept = mean(d[, y1]), color = 'darkorange', linetype = 1)

selection_plot
```

## Requirements of An Experiment 

David Reiley makes the case that an experiment is something where we intervene in the world to produce knowledge. This is essentially providing a definition and making an argument that this is the correct definition. One difficulty with argument through definitions is that reasonable people can disagree because their definitions, through their lived experience, just disagree. 

Here's the demonstrated proof: 

> Who in class is from the "midwest" broadly defined? Is Chicago-style pizza, pizza *per se*? 
> Who in class is from the east coast? Is Chicago-style pizza, pizza *per se*? 
> 
> Try not to make deep character judgments about your classmates. 

Green and Gerber, in *Field Experiments* make additional requirements of experiments. As they argue on page 45 of the textbook, in their view, experiments require: 

1. **Random Assignment** 
2. **Excludability** 
3. **Non-interference** 

What do each of these terms mean? Why is each necessary? 

- Did the experiment that Daniel conducted, described in *Power of Experiments* satisfy all of these requirements? For any that this experiment did not satisfy, what are the consequences for the scientific knowledge that the experiment generated. 
- Did the Nurses Health Study, described in the async, satisfy all of these requirements? For any that this experiment did not satisfy, what are the consequences for the scientific knowledge that the experiment generated. 