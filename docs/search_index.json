[["index.html", "Experiments and Causality Live Session Introduction", " Experiments and Causality The 241 Instructional Team1 2022-08-23 Live Session Introduction This is the live session work space for the course. Our goal with this repository, is that we’re able to communicate ahead of time our aims for each week, and that you can prepare accordingly. "],["blooms-taxonomy.html", "Bloom’s Taxonomy", " Bloom’s Taxonomy An effective rubric for student understanding is attributed to Bloom (1956). Referred to as Bloom’s Taxonomy, this proposes that there is a hierarchy of student understanding; that a student may have one level of reasoning skill with a concept, but not another. The taxonomy proposes to be ordered: some levels of reasoning build upon other levels of reasoning. In the learning objective that we present in for each live session, we will also identify the level of reasoning that we hope students will achieve at the conclusion of the live session. Remember A student can remember that the concept exists. This might require the student to define, duplicate, or memorize a set of concepts or facts. Understand A student can understand the concept, and can produce a working technical and non-technical statement of the concept. The student can explain why the concept is, or why the concept works in the way that it does. Apply A student can use the concept as it is intended to be used against a novel problem. Analyze A student can assess whether the concept has worked as it should have. This requires both an understanding of the intended goal, an application against a novel problem, and then the ability to introspect or reflect on whether the result is as it should be. Evaluate A student can analyze multiple approaches, and from this analysis evaluate whether one or another approach has better succeeded at achieving its goals. Create A student can create a new or novel method from axioms or experience, and can evaluate the performance of this new method against existing approaches or methods. "],["importance-of-experimentation.html", "Unit 1 Importance of Experimentation", " Unit 1 Importance of Experimentation Point Reyes National Seashore "],["core-questions.html", "1.1 Core Questions", " 1.1 Core Questions This course is about designing experiments that we run in the real-world. What is the value of making a causal statement? Why do we conduct experiments? This is a data science program. With enough data and a savvy enough model, can’t we just generate a causal statement that will be right? Can’t I generate a statement that converges in probability to the correct value? "],["learning-objectives.html", "1.2 Learning Objectives", " 1.2 Learning Objectives At the end of this live session, students will be able to Remember (or find) the goals of the course, the assessment structure, and the learning model. Define, in non-technical language, what it means for an action to cause an outcome. Understand the difference between a causal statement, and an association statement. Apply the framework of causal thinking against a series of studies to determine whether the study has achieved the goal that it intends. "],["class-introductions.html", "1.3 Class Introductions", " 1.3 Class Introductions "],["student-introductions-breakout-one.html", "1.4 Student Introductions [Breakout One]", " 1.4 Student Introductions [Breakout One] In a breakout room of between three and four students introduce yourself! Breakout One. A name story is the unique, and individual story that describes how you came to have the name that you do. While there may be many people are called the same thing, each of their name stories is unique. Please share: What is your name story? "],["student-introductions-breakout-two.html", "1.5 Student Introductions [Breakout Two]", " 1.5 Student Introductions [Breakout Two] In the same breakout room: Breakout Two. Like our names, the reasons that we joined this program, our goals and our histories are different. Please share: What is your data science story? How did you wind up here, in this room today? "],["course-plan.html", "1.6 Course Plan", " 1.6 Course Plan The course is built out into three distinct phases Part 1 Develops causal theory, potential outcomes, and a permutation-based uncertainty measurement Part 2 Further develops the idea of a treatment effect, and teaches how the careful design of experiments can improve the efficiency, and easy of analysis Part 3 Presents practical considerations when conducting an experiment, including problems that may arise, and how to design an experiment in anticipation of those problems. "],["course-logistics.html", "1.7 Course Logistics", " 1.7 Course Logistics bCourses Learning Modules attached to weeks Modules contain async lectures, coding exercises, and quizzes GitHub All the course materials are available in a GitHub repository We have protected the main branch, so you can’t do anything destructive Use that as empowerment! This is your class, propose changes that you would like to see! Github Classroom Assignments will all be applied programming assignments against simulated and real data All assignment code will be distributed through GitHub Classroom Gradescope All assignments will be submitted to Gradescope where we’ll read your solutions and provide scores and feedback 1.7.1 Learning model for the class The course assignments are designed to put what we have learned in reading, async, and live session into practice in code. In our ideal version of your studying, we would have you working hard together with your classmates in a study group on the assignments, coming to office hours to talk candidly about what is and isn’t working, and then every single student arriving at a full solution. 1.7.2 Feedback model for the class We want to get you feedback very quickly after you turn your assignments. We will release a solution set the day that you turn your assignment in We will hold a problem set debrief office hour the Friday (i.e. next day) after the problem set it submitted We will have light-feedback on your assignments within 7 days of when you submitted them. You should bring your assignment to office hours after you have turned it in so that we can talk about any differences between your approach, and the instructors approach. 1.7.3 Office hour model for the class We will hold office hours Sunday through Thursday at 5:30. We will hold more than 10 hours of office hours every week; they will all be recorded, and any student is welcome in any office hour "],["article-discussion.html", "1.8 Article Discussion", " 1.8 Article Discussion 1.8.1 Predict or Cause What are a few examples that Athey raises of causal questions masquerading as prediction questions? Which of these examples is the most surprising to you? Is there something that is common to each of these examples? Is this a general phenomenon, or is Athey very clever in picking examples? Said differently, is Athey making a clever argument or is a lot of what we do as data scientists actually causal work in disguise? 1.8.2 Do the suburbs make you fat? What is the causal claim being made in this article? If you had to draw out this causal claim, using arrows, what would it look like? Do you acknowledge the association that the authors present? Is there actually a difference between the BMI of people who live in cities and the suburbs? If you acknowledge the association, does that compel you to believe the causal claim? Why or why not? Name, and draw, five alternative confounding variables that might make you skeptical that the claimed relationship exists. (Optional) Name, and draw two mechanisms that might exist between suburbs and BMI. Why does the existence (or not) of these mechanisms not pose a fundamental problem to the causal claim that the authors make? At the conclusion of reading this paper, do you believe that there is a causal relationship between location and BMI? If so, what compels you to to believe this; if not, why are you not compelled to believe this? 1.8.3 Nike Shoes What is the causal claim being made in this article? If you had to draw out this causal claim, using arrows, what would it look like? Do you acknowledge the association that the authors present? Is there actually a difference in the finish time between people who are running with the Nike shoes vs. other shoes? If you acknowledge the association, does that compel you to believe the causal claim? Why or why not? What are some of the confounding relationships that the authors identify? (Can you name four?) How do they adjust their analyses once they acknowledge the confounding problem? At the conclusion of reading this paper, do you believe that there is a causal relationship between shoes and finish time? If so, what compels you to to believe this; if not, why are you not compelled to believe this? 1.8.4 What is Science: Feynman’s View In Cargo Cult Science, Richard Feynman poses a view of science that is about a seeking of the truth. What is Feynman’s view of science? What does he think makes something scientific? What are ways that individuals fool themselves when they are working as scientists? What are ways that individuals fool themselves when they are working as data scientists? How can we as (data) scientists, train ourselves not to be fooled?2 "],["apples-to-apples.html", "Unit 2 Apples to Apples", " Unit 2 Apples to Apples fruit salad "],["learning-objectives-1.html", "2.1 Learning Objectives", " 2.1 Learning Objectives At the conclusion of this week’s live session, student will be able to: Describe, using the technical language of potential outcomes, what it means for an input to cause an output. Describe the fundamental problem of causal inference. Apply iid sampling as a method of producing an unbiased, consistent estimator of a population. Prove that the average treatment effect estimator produces an unbiased, consistent estimator for the average treatment effect. "],["revisiting-ideas-of-science.html", "2.2 Revisiting Ideas of Science", " 2.2 Revisiting Ideas of Science Questions about epistemology are a classic question, and one that is particularly relevant not only at the School of Information where we have faculty and student whose work ranges from fields as diverse as computer science, psychology, sociology, law, and education – “What does it mean to know something?” We’ll note that this question is not only an academic question, because in our workplaces we need to know how to take the best course of action. In this course, we like to think of Science as a method of coming to know something. Think back to the reading and discussion from last week: For Feynman, what does it mean to be “Doing science?” Would Feynman say that data science, as we are practicing it, is a “science”? Would Feynman say that 205, or computer science is a science? Would Feynman say that 203 is a science? What about 251, 255, or 266? For Lakatos, what does it mean for something to be a part of a science? What does it mean for something to be a part of a psuedo-science? Is it as simple a view as Feynman espouses? Does the work that we conduct across the coursework in this program produce scientific knowledge as Lakatos see it? What do you think produces knowledge? Can a single conversation produce knowledge? Can a non-experimental study produce knowledge about a causal effect? Can an experiment fail to produce knowledge? If an experiment fails to reject some null hypothesis, does that mean that it has not produced any knowlege? "],["this-causes-that.html", "2.3 This Causes That", " 2.3 This Causes That What does it mean for an action to cause an outcome? Don’t worry about conducting the experiment, or any measurement concerns at this point, just engage with the concepts. 2.3.1 Damn fine coffee Suppose that you’re getting ready for class, and you want to make sure that you’re at your best. So, you drink a cup of water, eat a small snack, and brew a small pot of coffee for while you’re in class. Why do you do this? Presumably, you’re doing this because you like each of these things, but also because you’re interested in these things causing you to have a better class. If you framed this as as causal question, you might ask: If I drink a cup of coffee before class, will it cause me to be more alert? What does it mean for coffee to cause alertness? Does coffee cause everyone to become more alert? Does coffee have to affect everyone equally in order for you to say it causes alertness? Could coffee have no effect for some people, and you would still say it causes alertness? 2.3.2 Meditation for focus Suppose that you’re getting ready for class, and you want ensure that you’re at your best. So, you find a quiet place, and set your mind at ease with whatever form of meditation you think might be helpful. If I meditate before class, will it cause me to be more focused? What does it mean for meditation to cause focus? Does meditation cause everyone to become more focused? Does meditation have to affect everyone equally? Some people are frustrated by not being able to quiet their thoughts, and actually find meditation frustrating. Can this be true, and still believe that meditation causes focus? 2.3.3 Selling coffee and meditation Suppose that you’re an enterprising soul, and you want to sell a book about brewing coffee as a meditation. You reason that there must be a niche for this approach. To get the word out, you place a few flyers with tear off phone-numbers at the local yoga studios and tech incubators (good intuition to find those MIDS students). If shown a flyer for coffee-meditation, will it cause someone to take my training? What does it mean for for flyers to cause people to sign-up for the training? Does the flyer cause everyone to take the training? Does the flyer affect everyone equally? One might be a radical behaviorist (Skinner is perhaps the most famous in this line of thinking) that says, “In matters of human behavior, if I cannot see it, then I cannot reason or know about.” If this is your view, then you would simply stop your investigation (and reasoning) at the conclusion of your experiment. In many ways, experiments suited only to answer empirical, observable questions. These are the questions, and lens proposed by the radical behaviorist paradigm. 2.3.4 Limits of Behaviorist Reasoning If you accept only that coffee has this effect, and that it is measurable, are you able to translate this knowledge to a new context? Suppose that your experiments finds an effect of coffee on alertness: those who drink a cup of coffee are more alert in class. Suppose, though, that you’re out of coffee tonight. A radical behaviorist would simply say, “I know not what to drink then to increase my alertness.” 2.3.5 Reflecting on Causes Does anything unify questions of causes? When you think about {this} causing {that}, do you think about it at a population level, a smaller group level, or at the individual level? 2.3.6 Evaluating Value Is this an entirely academic exercise, the discussion of {this} causing {that}? Or, is there some value to thinking about things in these terms? Susan Athey, whom we read last week, seems to think that there is value in distinguishing between associations and causes. However, hers is a view that is generated by an academic; much like the views of David and David, and all of the live session instructors. We’re all academics, so maybe we’re being typical academic pedants. What is a case, perhaps that you read or wrote about for your first essay, mistakenly believing they had measured a causal effect? What would happen if they implemented the policy that is implicated in their study? Or, what would happen if they took action consonant with what their study purports to find? 2.3.7 Value of Theory Can you produce several theories (some of them might be silly) about why coffee might increase alertness in class? Proposed theory #1: Proposed theory #2: Proposed theory #3: Does Feynman’s approach to Science provide a method to adjudicate which of these theories is consonant with the evidence, and which are not consonant with the evidence? Does Lakatos’ approach to Science provide such a method? 2.3.8 Evaluating Theories What data might you be able to produce that would allow you to “drive a wedge” between the different theories? This ability to proactively deign an experiment to distinguish between theories is the goal you’re striving to achieve, and it is very hard to accomplish. "],["reading-discussion-the-power-of-experiments.html", "2.4 Reading Discussion: The Power of Experiments", " 2.4 Reading Discussion: The Power of Experiments The Power of Experiments starts the discussion of experimentation in the workplace with what is, for the course instructors, a uniquely pedestrian example, increasing contributions to taxes. In particular, Her Majesty’s Revenue and Customs sends different versions of a letter to British taxpayers, and observes that different language leads to different amounts of taxes being paid. 2.4.1 Chapter One: The Power of Experiments Is it actually a “big-deal” to increase tax compliance by 2 percentage points? On page five, the book identifies five “one-liners” that HMRC chose to send to taxpayers: Nine out of ten people pay their tax on time.” Nine out of ten people in the UK pay their tax on time. Nine out of ten people in the UK pay their tax on time. You are currently in the very small minority of people who have not paid us yet.” Paying tax means we all gain from vital public services like the NHS, roads and schools. Not paying tax means we all lose out on vital public services like the NHS, roads, and schools. Which of these sentences would be the most effective at getting you to pay your taxes? Which do you think will be most effective, overall, at generating tax compliance? Why? How willing are you to make a million pound bet that you’re correct? Some of your instructors are vegetarians. None of them, however, has previously made an argument for why everyone should be vegetarian based on the example of Daniel and his study of diet and divine intervention. What about the study that Daniel conducted produces evidence that you think is useful for evaluating diet? What are the limitations that you see in this study? The book lists several, but there are other issues, along the lines of the exclusion restriction that Field Experiments identifies. In order for Pasteur to be declared the winner of the vaccine argument, the observers said that every control group sheep had to die and every treatment group (i.e. vaccine-receiving) sheep had to live. Is this a fair burden of proof? Do the frequentest tests that we developed in 203 and are going to use here in 241 set a higher or lower bar than Pasteur faced? What are the merits of a relatively higher or lower bar? 2.4.2 Chapter Two: The Rise of Experimetnts in Psychology and Economics Freud is noted as being specifically against experimentation. But, PoE then goes on to write, “[Freud’s] big ideas inspired entire fields of psychological research. Including the notion that unconscious processes shape our judgement and behavior, psychological disorders are rooted in the mind rather than the body; and that sexual urges and behavior are worthy of study” (p. 19). Some of the theories that Freud promulgated were found to have evidence that was consistent with the theory; some of these theories could not produce evidence to support the theory; and many were outright contradicted by the evidence. Is there value in being an “idea person”? How would you ever know if your ideas were actually right if you’re unwilling to evaluate them? What, if any, are the limitations of experimenting without any “big ideas” to ground your experiments? Behaviorists (Skinner is the leading behaviorist) make a compelling argument: “One cannot directly observe what is happening in the mind of a person.” A classic implication of this argument for behaviorists is that only that which is empirically observable is reasoned about. “Why does the rat avoid getting shocked? Does it really matter?” “Why does the child want a cookie? Does it really matter why?” Is this position reasonable for you to take as you navigate your own life? If you spoke with a therapist or a coach and said, “I’ve been feeling stressed over the past several weeks,” would be satisfied with a mindful answer like, “Well, let’s acknowledge those feelings and hold them for a moment” or would you want to reason further about why you feel stressed? What are the types of things in people’s heads that you think we can profitably reason about; what are the types of things in people’s heads that we cannot reason about? Is there something that is common to those that we can or cannot work with? The experiments of Milgram and Zimbardo are widely identified as the reason that human-subjects review boards no exist. These review boards serve as an external review that keeps researchers from inflicting harm to individuals that is not outweighed by societal or scientific benefits. What did Milgram and Zimbardo do to their subjects? By talking continuing to talk about these experiments nearly fifty years after they were conducted – even if we are talking about them negatively – are we adding to the fame of these researchers? (For those interested in inside baseball, Zimbardo was the president of the American Psychology Association in 2002, and was awarded a lifetime achievement award from his discipline.) How should we learn and react to work that shouldn’t have been conducted in the first place? Kahneman and Tversky propose that individuals think about expected values differently depending on whether they are thinking in the domain of gains or the domain of losses. They come to this theory through the, now cringe-worthy, Asian Disease Problem: In the positive frame, they ask the question: Imagine that the US is preparing for the outbreak of an unusual Asian disease that is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. If Program A is adopted, 200 people will be saved. If Program B is adopted, with a 1/3 probability 600 will be saved and with a 2/3 probability nobody will be saved. The authors also present a countervailing pair of scenarios framed in terms of losses If Program C is adopted, 400 people will die. If Program D is adopted, there is a one-third probability that no one will die, and a 2/3 probability that everyone will die. Clearly, all these programs have the same expected number of deaths; but, people can disagree about which of these is the program that we should pursue. Just ask as a poll in the class; and, ask people to justify their beliefs. 2.4.3 The Rise of Behavioral Experiments in Policymaking PoE points out that experiments abound in policy making. Part of this stems from a truthful ignorance of the optimal policy to pursue. Another part of this stems from the ability of policy makers to make decisions by fiat that affect a large number of people. Does this justification for experiments align with your current understeanding of the landscape in human-facing data science? In a section titled The nuance behind behavioral insights the authors state a series of three caveats: Context matters Design choices matter Unintended consequences abound What do they mean when they raise the three points? We are going to ask you to justify conducting experiments by staking out an extreme point of view, and asking you to convince us that this point of view is so extreme that it cannot be justified. “Writing that context matters, design choices matter, and unanticipated consequences abound is little more than writing that experiments cannot produce any more useful insights than the theories of Freud. As a result, there is little reason to conduct any experiments because what we learn will be highly contextualized, affected by very small implementation choices, and may generate as many (or more) negative outcomes as positive outcomes.” "],["potential-outcomes.html", "2.5 Potential Outcomes", " 2.5 Potential Outcomes Potential outcomes are a system of reasoning, and a corresponding notation, that allow us to talk about observable and un-observable characteristics of the world. What is your position on ontology? What does it mean for something to exist? Does Field Experiments, as a textbook, exist? Do Don Green and Alan Gerber, the authors of the textbook that we’re reading, exist? Does David Reiley, the slower-talking Davids in the async, exist? Do I, your section, instructor, exist (or am I a deep fake in this room with you)? Can a concept exist, even if you can’t hold it? Even if you haven’t seen it? 2.5.1 Defining Potential Outcomes For each of the following sets of notation: (1) Read the notation aloud, not as “Y sub i zero”, but instead as “The potential outcome to control …”. \\(Y_{i}(0)\\): \\(Y_{i}(1)\\): \\(E[Y_{i}(0)]\\): \\(E[Y_{i}(1)]\\): \\(E[Y_{i}(0)|D_{i}=0]\\): \\(E[Y_{i}(1)|D_{i}=1]\\): \\(E[Y_{i}(0)|D_{i}=1]\\): \\(E[Y_{i}(1)|D_{i}=0]\\): Which of these concepts that you have just read aloud exist? Can a concept exist, even if you can’t hold it? Even if you can’t see it? "],["using-independence.html", "2.6 Using Independence", " 2.6 Using Independence Suppose that you have a random variable that is defined as the function, \\[ Y = \\begin{cases} \\frac{1}{10} &amp; ,0 \\leq y \\leq 10 \\\\ 0 &amp; \\text{, otherwise} \\end{cases} \\] What is the expected value of this function? \\[ \\begin{aligned} E[Y] &amp;= \\int_{0}^{10} y \\cdot f_{y}(y) \\ dy \\\\ &amp;= \\int_{0}^{10} y \\cdot \\frac{1}{10} \\ dy \\\\ &amp;= \\frac{1}{10}\\int_{0}^{10} y \\ dy \\\\ &amp;= \\left.\\frac{1}{10} \\cdot \\frac{1}{2} y^2\\right|_{0}^{10} \\\\ &amp;= \\left.\\frac{1}{20} y^{2} \\right|_{0}^{10} \\\\ &amp;= \\frac{1}{20} \\cdot \\left[(100) - (0) \\right] \\\\ &amp;= \\frac{1}{20} \\cdot 100 \\\\ &amp;= \\mathbf{5} \\end{aligned} \\] Why is the expected value a good characterization of a random variable? If you wanted to write down an estimator to produce a summary statistic for \\(Y\\) given a sample of data, what properties do the following estimators possess: \\(\\hat{\\theta}_{1} = y_{1}\\) \\(\\hat{\\theta}_{2} = \\frac{1}{2} \\displaystyle\\sum_{i=1}^{2} y_{i}\\) \\(\\hat{\\theta}_{3} = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^{N} y_{i}\\) \\(\\hat{\\theta}_{4} = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{N} y_{i}\\) conduct_sample &lt;- function(size) { runif(n=size, min=0, max=10) } theta_1 &lt;- function(data) { # take the first element } theta_2 &lt;- function(data) { # sum the first two elements and divide by two } theta_3 &lt;- function(data) { # sum the sample, and divide by 1 less than the sample size } theta_4 &lt;- function(data) { # sum the sample, and divide by the sample size # honestly, just use the mean call. # clearly, this is a silly function to write, since you&#39;re just # providing an alias, without modification, to an existing function. mean(data) } theta_4(conduct_sample(size=100)) ## [1] 5.363375 Just to put a fine point on it: **What estimator properties does the sample average provide, and why are these desirable?” "],["use-randomization-to-produce-independence.html", "2.7 Use Randomization to Produce Independence", " 2.7 Use Randomization to Produce Independence How can we use the independence that is induced by “random assignment to treatment” combined with the sample average estimator to produce an estimate of an otherwise very difficult concept to measure? "],["theoretical-justification.html", "2.8 Theoretical Justification", " 2.8 Theoretical Justification Before we show that this very simple ATE estimator work against a sample of data, it is worth reasoning about whether we can guarantee that it works in a general case. If we can show that it works in a general case, then any specific case inherits that guarantee. However, if we can only reason thorugh the existence of a single examlpe, it is not a sufficient argument to compell us to believe that it must hold for all cases. Here’s an example, “Behold! I see a black sheep! Therefore all sheep are black.” This doesn’t make sense, and it is not a logically sound argument. However, if you say, “All sheep say, ‘Baaah!’ This is a black sheep, so it must say ‘Baah!’” is a logically sound arugment, so long as the antecedent is, in fact true. When we’re proving something, we’re proving that the antecedent to this statement is generally true. For anyone who took a symbolic logic course in, this method of argument might be marked down as \\(\\forall X \\implies \\exists X\\), whereas \\(\\exists Y \\not\\Rightarrow \\forall Y\\). What concepts compose \\(\\tau_{David}\\)? What concepts compose \\(\\tau_{i}\\)? Is there any reason to believe that \\(\\tau_{David\\ Reiley} = \\tau_{David\\ Broockman}\\)? Is there any reason to believe that \\(\\tau_{i} = \\tau_{j}\\), where \\(j \\neq i\\)? Could \\(\\tau_{i} = \\tau{j}\\)? What is the fundamental problem of causal inference? The proof for this argument is also made in Field Experiments, on or about page 30 of the text. However, in our view, the authors don’t give enough room to fully develop this proof, and so we skipped right past it the first time that we read the chapter. Begin our proof with the statement for what a treatment effect is, \\(\\tau_{i}\\). \\[ \\begin{aligned} \\tau_{i} &amp;= Y_{i}(1) - Y_{i}(0) &amp; Definition \\\\ ATE &amp;= E[\\tau] &amp; Definition \\\\ &amp;= E[Y(1) - Y(0)] &amp; Substitution \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ &amp;= \\\\ \\end{aligned} \\] "],["simulation-example.html", "2.9 Simulation Example", " 2.9 Simulation Example Now, let’s work through an example that shows this works not only in the math, but also in the realized, i.e. sampled, world. To begin with, lets work with a very simple sample that has 100 observations, potential outcomes to control are uniformly distributed between 0 and 1 and every single unit has a potential outcome to treatment that is 0.25 units larger than their potential outcomes to control. make_simple_data &lt;- function(size=100) { require(data.table) d &lt;- data.table(id = 1:100) d[ , y0 := runif(.N, min = 0, max = 1)] d[ , y1 := y0 + .25] return(d) } d &lt;- make_simple_data(size=100) d[1:5] ## id y0 y1 ## 1: 1 0.3058068 0.5558068 ## 2: 2 0.2968694 0.5468694 ## 3: 3 0.0483885 0.2983885 ## 4: 4 0.4381437 0.6881437 ## 5: 5 0.6983875 0.9483875 In this world, we’ve taken a sample of 100 individuals, and at this point, each of those individuals that we’ve sampled has both a potential outcome to control and also a potential outcome to treatment. We haven’t talked at all about measurement yet; we’re just asserting that both of these potential outcomes exist for each person. Essentially, this stage of creating the sample is the same as bringing people in the door to your experiment. If you were running this in the laboratory, you’d literally think of this as sitting your subjects down at their chairs, getting ready to begin their task. Is randomly sampling people to be a part of your experiment sufficient to ensure that your experiment produces an unbiaed, consistent estimate of the true treamtent effect? Suppose that for each unit, you then toss a coin, placing the subject either into treatment or control based on the result of that coin flip. Does this coin flip ensure that you have the same number of units in treatment as control? Does this matter to you? Why or why not? Are there other ways that you could assign individuals to treatment an control, rather than through a simple-randomization process? What are the relative merits or limitations of each of the methods? Are some of these methods more random than others? Or, are all things that are random equal in their randomness? 2.9.1 Assign to Treatment and Control d[ , experimental_assignment := sample(0:1, size = .N, replace = TRUE)] d[1:5] ## id y0 y1 experimental_assignment ## 1: 1 0.3058068 0.5558068 1 ## 2: 2 0.2968694 0.5468694 1 ## 3: 3 0.0483885 0.2983885 1 ## 4: 4 0.4381437 0.6881437 1 ## 5: 5 0.6983875 0.9483875 1 As as comparison, suppose that instead of randomly assigning individuals into treatment and control we allowed individuals to select into treatment and control. And suppose that people with the lowest potential outcomes to control opt to take the treatment. You might think of this as being something like, “The people who are the most tired are the most likely to drink a cup of coffee before they start class,” if an example helps you ground this. Specifically, suppose that any unit that has a potential outcome lower that 0.33 opts to take the treatment. d[ , observational_selection := ifelse(y0 &lt; .33, 1, 0)] d[1:5] ## id y0 y1 experimental_assignment observational_selection ## 1: 1 0.3058068 0.5558068 1 1 ## 2: 2 0.2968694 0.5468694 1 1 ## 3: 3 0.0483885 0.2983885 1 1 ## 4: 4 0.4381437 0.6881437 1 0 ## 5: 5 0.6983875 0.9483875 1 0 These represent two different ways that you might conduct your research, each time with the same subject pool. Of course, in reality you probably would not be able to run these two studies at the same time, but since this is a simulation, we can stretch the confines of reality just a little bit. first_plot &lt;- ggplot(data=d) + geom_point(aes(x = id, y = y0), color = blue) + geom_point(aes(x = id, y = y1), color = gold) first_plot What’s actually happening in this? It might be more clear if we add arrows to this plot to show. first_plot + geom_segment( aes(x = id, xend = id, y = y0, yend = y1), arrow = arrow(ends = &#39;last&#39;, length = unit(0.1, &quot;inches&quot;), type = &#39;closed&#39;), color = &#39;grey70&#39; ) Even though these potential outcomes exist for all the units, is it possible to actually see them for all the units? How do we go about showing, and then measuring the potential outcomes to control for a set of units? How about the potential outcomes to treatment? second_plot &lt;- ggplot(data = d) + geom_point( aes(x = id, y = y0, size = 1 - experimental_assignment), color = blue) + geom_point( aes(x = id, y = y1, size = 0 + experimental_assignment), color = gold) + labs( title = &#39;Treatment and Control Assignment&#39;, size = &#39;Treatment or Control&#39; ) second_plot What are the averages of these samples that have been assigned to treatment? third_plot &lt;- second_plot + geom_hline( yintercept = mean(d[experimental_assignment==0, y0]), color = blue, linetype = 2) + geom_hline( yintercept = mean(d[experimental_assignment==1, y1]), color = gold, linetype = 2) third_plot Even though we aren’t able to see it, can we reason about what the sample average would be if we could see both of an individual’s potential outcome to treatment and control? Is there a guarantee that the sample should be the same as the feasible realization? Should they be close? What property from 203 provides this guarantee? third_plot + geom_hline(yintercept = mean(d[, y0]), color = blue, linetype = 1) + geom_hline(yintercept = mean(d[, y1]), color = gold, linetype = 1) Put it all together, what has this little demo shown? 2.9.2 What if there is selection? What if, rather than being assigned to treatment and control, instead individuals had been able to opt into treatment and control? Produce only the last plot, but this time for the observational, or selected data. selection_plot &lt;- ggplot(d) + geom_point( aes(x = id, y = y0, size = 1 - observational_selection), color = blue) + geom_point( aes(x = id, y = y1, size = 0 + observational_selection), color = gold) + geom_hline( yintercept = mean(d[, y0]), color = blue, linetype = 1) + geom_hline( yintercept = d[observational_selection == 0, mean(y0)], color = blue, linetype = 2) + geom_hline( yintercept = mean(d[, y1]), color = gold, linetype = 1) + geom_hline( yintercept = d[observational_selection == 1, mean(y1)], color = gold, linetype = 2) + labs( title = &#39;Observational Selection into Treatment&#39;, size = &#39;Treatment or Control&#39; ) selection_plot "],["requirements-of-an-experiment.html", "2.10 Requirements of An Experiment", " 2.10 Requirements of An Experiment David Reiley makes the case that an experiment is something where we intervene in the world to produce knowledge. This is essentially providing a definition and making an argument that this is the correct definition. One difficulty with argument through definitions is that reasonable people can disagree because their definitions, through their lived experience, just disagree. Here’s the demonstrated proof: Who in class is from the “midwest” broadly defined? Is Chicago-style pizza, pizza per se? Who in class is from the east coast? Is Chicago-style pizza, pizza per se? Try not to make deep character judgments about your classmates. Green and Gerber, in Field Experiments make additional requirements of experiments. As they argue on page 45 of the textbook, in their view, experiments require: Random Assignment Excludability Non-interference What do each of these terms mean? Why is each necessary? Did the experiment that Daniel conducted, described in Power of Experiments satisfy these three requirements? For any of these requirements that David’s experiment did not satisfy, what are the consequences for the scientific knowledge that the experiment generated? Did the Nurses Health Study, described in the async, satisfy all these three requirements? For any that this experiment did not satisfy, what are the consequences for the scientific knowledge that the experiment generated? 2.10.1 Meta-Questions Can an experiment generate scientific knowledge about a causal effect, even without satisfying all of these requirements? Is it guaranteed to produce scientific knowledge about a causal effect? What then, justifies the use of experiments to measure causal effects? "],["quantifying-uncertainty.html", "Unit 3 Quantifying Uncertainty ", " Unit 3 Quantifying Uncertainty "],["learning-objectives-2.html", "3.1 Learning Objectives", " 3.1 Learning Objectives At the end of this week’s live session, students will be able to Understand the sharp null, and how to apply it in an argument using randomization inference. Describe how randomization creates uncertainty, and assess how this uncertainty differs from that in Frequentist paradigm Apply the sharp null and randomization inference to data Assess the assumptions necessary for Frequentist inference to produce nominal coverage on confidence intervals; assess the assumptions necessary for randomization inference to produce nominal coverage on confidence intervals; and, evaluate which of the two approaches is appropriate given a set of data. Describe the concept of statistical power and what it means in the context of conducting an experiment. "],["power-of-experiments.html", "3.2 Power of Experiments", " 3.2 Power of Experiments 3.2.1 Five Key Barriers to Experimentation Power of Experiments identifies five key barriers to experimentation in companies: Not enough participants. How can it be that even a huge, digital company (i.e. Uber) might not have enough participants to conduct an experiment? Randomization can be hard to implement. This is not to be taken lightly; because in students essays this week, nearly every experiment proposed was of the form, “Randomly assign people to…”. What might make it hard to randomize? Experiments require data to measure their impact. This should ring of 201 conversations, but what is the concept that you would ideally like to measure about the impact of a policy? And, what instead are you able to measure? How much conceptual slippage is there between your conceptual definition and your data? Under-appreciation of decision-makers unpredictability. Do we actually have a theory about what people will do? How sure are we that the theory is correct? Overconfidence in our ability to guess the effect of an intervention. 3.2.2 Experimental Ethics There is a very, very strong norm that academic researchers who conduct experiments need to pass their interventions, data collection, and procedures through a review board. This review board expects researchers will weigh the costs borne by the participants of an experimental study against the potential benefits to science from learning the results of this experiment. In some cases, these boards determine that the costs are too high; nobody should be subject to those costs, no matter the scientific merits. In other cases, these boards will allow potentially costly actions to be taken, some that might even harm participants in the short-run. While it is quite unlikely that a review board would still approve either Milgram’s or Zimbardo’s infamous experiments, there are still many experiments that might harm participants. Is this OK? What are the tradeoffs, or goals that you would like to balance in an experiment? A research team at Facebook (as your instructors if they have any juicy details about this case) was interested in the effects of their platform on its user’s emotions. In pursuit of this question, they conducted an experiment – they intentionally manipulated the environment – to post more or fewer positive and negative posts. Is this OK? What are the tradeoffs, or goals that you would like to balance in this experiment? "],["statistical-uncertainty-randomization-inference-style.html", "3.3 Statistical Uncertainty – Randomization Inference Style", " 3.3 Statistical Uncertainty – Randomization Inference Style When we are working with a sample of data, estimates produced by an estimator might change – sometimes being higher than the true value, other times lower than the true value. In Frequentist inference, we understand the variance in these estimates as sampling based variance of the sample estimator. In this week, we present a different inferential paradigm, Randomization Inference. In randomization inference, there is no uncertainty about the parameter estimate that is generated in the experiment: The estimate that we observe is the estimate that we observe. Uncertainty, instead, comes from the acknowledgment that different randomization could have been realized, even from within the same sample. "],["stating-the-sharp-null.html", "3.4 Stating the sharp null", " 3.4 Stating the sharp null Suppose that you are evaluating the effect of coffee on students’ alertness in class. You reason that drinking coffee will increase students’ alertness in class. “Damn Fine Coffee.” Continue with our idea of an experiment to evaluate if coffee produces alertness in class. Here, we are going to further develop this notional experiment into something that we might actually be able to conduct. What is the sharp null hypothesis that is at risk in this investigation? How, if at all, does this sharp null differ from the null hypothesis you might be more familiar with? Is the sharp null hypothesis a concept that ever makes sense? Is the sharp null hypothesis a concept that is ever, actually, true? "],["randomization-inference.html", "3.5 Randomization Inference", " 3.5 Randomization Inference 3.5.1 Stating the process of Randomization Inference Randomization inference is a method of understanding the variability of results in an experiment that you have conducted. It specifically acknowledges several facts: The sample of data that you collected or used in your experiment is, quite simply, the sample of data that you collected for your experiment. There might be a larger population; there might be an infinite population; or, there might not. The observed outcomes that you observe are, quite simply, the outcomes that you observed. There is no uncertainty about having seen these. When the experiment assigned some units to treatment and others to the control, it revealed some outcomes, for some people. Specifically, it revealed the potential outcoems to treatment, denoted \\(Y_{i}(1)\\) for those who were assigned to the treatment group and the potential outcomes to control, denoted \\(Y_{i}(0)\\) for those who were assigned to the control group. The experimenter chose one out of many possible treatment assignments. If the sharp null hypothesis were to be true (note the subjunctive verb tense there) then, the particular revelation of potential outcomes to treatment and control are inconsequential. Despite seeing only half the data (referred to as the Fundamental Problem of Causal Inference) we actually posess all the data. After all, if the sharp null were true, \\(Y_{Alex}(1) = Y_{Alex}(0)\\), and \\(Y_{David}(1) = Y_{David}(0)\\), \\(Y_{i}(1) = Y_{i}(0)\\) for all of the \\(i = {1, \\dots, N}\\) people who are a part of the experiment. 3.5.2 Questions about Randomization Inference Where does uncertainty come from in an experiment that is evaluated using randomization inference? How is the ATE estimand defined? What is the feasible method that we use to write down an estimator (call it \\(\\theta\\)) for this quantity? Which of the following properties does this feasible method possess? Unbiasedness: \\(E[\\theta] = ATE\\) Convergence: \\(\\theta \\overset{p}\\rightarrow ATE\\), where \\(\\overset{p}\\rightarrow\\) means converges in probability Efficiency: The mean squared error of \\(\\theta\\) is either (i) smaller than some other estimator, or (ii) as small is theoretically possible. "],["applying-randomization-inference.html", "3.6 Applying Randomization Inference", " 3.6 Applying Randomization Inference 3.6.1 Make Data set.seed(1) d &lt;- data.table( id = 1:100, D = rep(0:1, each = 50), Y = c(rnorm(n=50, mean=0, sd=2.5), rnorm(n=50, mean=1, sd=2.5)) ) 3.6.2 Plot Data In the following plot, are you able to assess whether there is a treatment effect simply by looking at the distributions? ggplot(d) + aes(x=Y, fill=as.factor(D)) + geom_density(alpha=0.5) + labs( x = &#39;Distribution of outcomes&#39;, y = NULL, title = &#39;Distribution of outcomes, by treatment&#39;, fill = &#39;Treatment\\nAssignment&#39;) + scale_fill_manual( values = c(&#39;#003262&#39;, &#39;#FDB515&#39;) ) 3.6.3 Classic Test If you were to write a classic test against this data, given what you know about how it was generated, what would be the classic test? What do you learn from this test, and what is the interpretation? d[ , t.test(Y ~ D)] ## ## Welch Two Sample t-test ## ## data: Y by D ## t = -2.309, df = 95.793, p-value = 0.02309 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1.9381728 -0.1462181 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.2511207 1.2933161 3.6.4 Randomization Inference Test Now, instead suppose that you were to conduct the randomization inference. What are the steps to the algorithm for producing a result using randomization? State the null hypothesis Compute the statistic of interest using the observed data Fill in data, under the statement of the null hypothesis Permute the treatment assignment labels to generate a new sample of the treatment assignment vector, and then estimate the statistic of interest Repeat the permutation and estimation (step 4) process repeatedly to sample from the randomization inference distribution of the statistic Examine randomization inference distribution ## 1. The sharp null is that tau = 0 ## 2. Compute the statistic of interest true_ate &lt;- d[ , .(group_mean = mean(Y)), keyby = .(D)][ , group_mean[D==1] - group_mean[D==0]] ## 3, 4, 5. Permute the treatment assignment labels and repeatedly compute the statistic of interest ri_distribution &lt;- replicate( n=10000, expr = d[ , .(group_mean = mean(Y)), keyby = .(ri_treatment = sample(D))][ , group_mean[ri_treatment==1] - group_mean[ri_treatment==0]] ) # 6. Examine distribution ggplot() + geom_density(aes(x=ri_distribution), fill = &#39;#003262&#39;, alpha = 0.5) + geom_vline(xintercept = true_ate, color = &#39;#FDB515&#39;) + labs( x = &#39;Randomization Inference Distribution and Estimated ATE&#39;, y = NULL, title = &#39;Randomization Inference Distribution and Estimated ATE&#39;) How much of the randomization inference is more extreme than the treatment effect? ri_p_value &lt;- mean(abs(ri_distribution) &gt; abs(true_ate)) ri_p_value ## [1] 0.0226 Notice that 0.023 of the randomization inference distribution is more extreme than the observed treatment effect. How does this compare to the t-test p-value that we calculated above? "],["comparing-randomization-inference-and-frequentist-inference.html", "3.7 Comparing Randomization Inference and Frequentist Inference", " 3.7 Comparing Randomization Inference and Frequentist Inference If both Randomization Inference and Frequentist Inference produce similar p-values, what is utility in learning another set of methods for communicating estimator-based uncertainty? What are the requirements (frequently referred to as “assumptions”) that are necessary for the Frequentist paradigm to provide guarantees? What happens if these guarantees are not, in fact, satisfied or true in the data generating process? How do you react, respond, or address those problems? If data is not sampled iid, is it sufficient to simply note that limitation (frequently referred to as an “assumption violation”) and report whatever p-value you report? How affected is this p-value by the violation? How do you know this? What does it mean for the p-value to be affected by this violation? (Recall that a p-value is just a random variable that is produced through a series of summarizing transformations and then a comparison against a reference distribution.) 3.7.1 Donations to a political campaign In Field Experiments Green and Gerber provide some useful (hypothetical) data about donations to a political campaign. The data is defined in the following way, \\(D\\) is an indicator for whether the potential donor is assigned to treatment or control, and \\(Y\\) is the outcome of how much the potential donor actually gave. Let us provide a little bit more back story, that is necessary for the example to work, fully. Suppose that a progressive political candidate was hosting a fundraiser in Berkeley and has to make a choice about what to serve the attendees at the fundraiser. In the \\(D = 0\\) group, suppose that the candidate elects to serve a hippie-vegetarian staple, tofu sauteed in Bragg’s liquid aminos. (It is Berkeley after all.) In the \\(D=1\\) group, suppose that the candidate decides to be a little more, well, progressive in their vegetarian food offerings and instead serves Gado-Gado from Katzen’s The Enchanted Broccoli Forest. (Still Berkeley… .) After dinner, and the requisite drum-circle, attendees to this shin-dig are asked to donate to the candidates re-election efforts. Every attendee is expected to contribute something – social norms rule out failing to donate when the collection plate is passed – but the amount donated is at the discretion of the attendee. d &lt;- data.table( id = 1:20, D = rep(0:1, each = 10), Y = c(500, 100, 100, 50, 25, 25, 0, 0, 0, 0, ## tofu diners 25, 20, 15, 15, 10, 5, 5, 0, 0, 0) ## gado gado diners ) With this data, conduct a t.test to assess whether the choice of dinner affects the amount donated to the campaign. What is your null-hypothesis (be specific), what is your rejection criteria, and do you reject or fail to reject this null hypothesis under the t-test framework. ## Null Hypothesis: ## Rejection Criteria: ## Conduct the Test Here: ## Conclusion: With this data, use randomization inference to assess whether the choice of dinner affects the amount donated to the campaign. What is your null-hypothesis (be specific), what is your rejection criteria, and do you reject or fail to reject this null hypothesis under the t-test framework. ## Null Hypothesis: ## Rejection Criteria: ## Conduct the Test Here: ## Conclusion Characterize the distribution of the sharp-null distribution of treatment effects. Talk about what, if anything, is notable about it, and what components of the data might be leading to any patterns that you note. How many of the randomization inference loops are larger than the treatment effect that you calculated? How would you use this statement to construct a one-sided test, and an associated p-value? How many of the randomization inference loops are more extreme (:metal:) than the treatment effect that you calculated? How would you use this statement to construct a two-sided test, and an associated p-value? Compare the two-sided p-value against the p-value that you generate from a two-tailed t-test. If these p-values are the same, would this be a positive or a negative characteristic of randomization inference? If these p-values are different, why would they be different? Don’t go looking all over hill-and-dale for the call for a t-test, it is at t.test. Which of the two of these inferential methods do you prefer, randomization inference or a t-test, and why? Ease of use is not an acceptable answer. "],["statistical-power.html", "3.8 Statistical Power", " 3.8 Statistical Power What is statistical power? Why is it particularly relevant to consider statistical power when you are thinking about conducting an experiment? What would happen if you were to conduct an experiment that has only an achieved power of 0.1? What would you learn if you were to fail to reject the sharp-null hypothesis? What would you learn if you were to reject the sharp-null hypothesis? make_data &lt;- function( sample_size = 100, potential_outcome_to_control_mean = 10, potential_outcome_to_control_sd = 2, treatment_effect = 1, sd_treatment = 2) { ## this is a function to make data to simulate the power of a test } test_data &lt;- function(data, treatment_indicator, outcome) { } ## p_values &lt;- replicate(n = 1000) "],["blocking-and-clustering.html", "Unit 4 Blocking and Clustering", " Unit 4 Blocking and Clustering When assigning treatment to units, unless there are restrictions created by the researcher, any of the treatment assignment vectors are equally probable. Blocking and clustering are ways of restricting the treatment assignments to a subset of the whole schedule of possibilities. Blocking is a method of creating “blocks”, or groups, of units that are similar along one or more dimensions and then creating a full random assignment within each of those similar groups. Through careful design, blocking can generate power or nuance for an experiment without any extra marginal costs for paying for additional units of treatment. Clustering is a circumstance that arises from a state of the world that requires you to assign several similar units to the same condition, be it treatment or control. Through careful design, clustering might not hamper the power of an experiment; though realizing the necessity of a clustered design is typically met with the following statement, “@#$%, we’ve got to cluster.” "],["learning-objectives-3.html", "4.1 Learning Objectives", " 4.1 Learning Objectives At the end of this week, student will be able to Recognize when there is the potential to block random assign in their experiment, and remember why block random assignment beneficial. Recognize when they are required to cluster random assign – either due to a pragmatic (i.e. real-world) limitation, or to avoid violating the requirement that units not interfere with one another – and identify ways that they can mitigate the reduction-in-power that arises from the need to cluster. Distinguish between the circumstances that lead to blocking and clustering. Analyze both blocked and clustered experiments using the appropriate test, and generating statements of certainty and uncertainty using randomization inference. "],["setting-terms-blocking.html", "4.2 Setting terms: Blocking", " 4.2 Setting terms: Blocking What does it mean to block randomize? Does the elimination of some randomization mean that the randomization is not longer, well, random? Relative to when treatment is administered, when are we able to block? Why are we not able to block after we’ve assigned treatment? "],["math-block-random-assignment.html", "4.3 Math: Block random assignment", " 4.3 Math: Block random assignment In equation 3.6 (on page 61) of Field Experiments Green and Gerber write, \\[ \\widehat{SE} = \\sqrt{\\frac{\\widehat{Var}(Y_{i}(0))}{N-m} + \\frac{\\widehat{Var}(Y_{i}(1))}{m}} \\] When we block randomize, we’re essentially creating smaller groups of units and producing an estimate of the variance within each of those smaller groups of units. How do the authors arrive at the following formula for a block randomized standard error? \\[ \\widehat{SE}(\\widehat{ATE}_{blocked}) = \\sqrt{\\sum_{j=1}^{J}\\left(\\frac{N_{j}}{N}\\right)^2 * \\widehat{SE}^2(\\widehat{ATE}_{j})} \\] Specifically, why are we squaring the scaling parameter \\(\\frac{N_{j}}{N}\\)? If you look at this summation, what has to happen to the variance within the groups, relative to the size of the groups, in order for blocking to actually increase power? Is it possible that you block, without increasing power, even if the blocking variable is actually useful? Green and Gerber, in equation 3.10, write that the overall \\(ATE\\) of the population is: \\[ ATE = \\sum_{j=1}^{J} \\frac{N_{j}}{N} ATE_{j} \\] What does this equation “feel like”? Does that seem reasonable? Why or why not? Why might it be a good idea to have different rates of assignment to treatment within different blocks? Consider the following example: Suppose that you are looking at an experiment among your whole user base, and you are considering changing the “check out flow” (we have not idea what that might mean either…) for this group. Some of the users are really likely to purchase, while others are very unlikely to purchase. Does it make sense to block randomize based on this prior purchase history? Are there any, reasonable business reasons to not make the treatment assignments be 50% treatment and 50% control in both of the populations? What would happen if you randomized 10% of the “high value” customers into treatment and 50% of the low value customers into treatment. But, then you forget (or lost) that table of whether they were “high” or “low” value customers. What would be the consequence to your treatment effect estimate? "],["intuition-block-random-assignment.html", "4.4 Intuition: Block Random Assignment", " 4.4 Intuition: Block Random Assignment To discuss the idea of blocking, consider the working example that David and David present in the async lectures: Eating too much tofu (aka the Berkeley diet) might increase decrease one’s brain function, leading to decreased performance on cognitive tests, lower brain weight, and cause ventrical enlargment of the brain. Don’t ask your instructors what any of that medical jargon might mean. It isn’t our field! But, these are real claims made by a group of researchers in an observational nutrition study titled “Brain Aging and Midlife Tofu Consumption.” this is your brain on tofu Suppose that, motivated by your distaste for bunk, casual causal claims about diet, and taste for tofu, you decide to conduct a real experiment among your friends, families, and classmates to determine the actual impacts of tofu on diet. set.seed(1414) sim_normal_study &lt;- function(treatment_effect=0) { ## this function will create a &quot;world&quot; to analyze using an experiment, ## then, it will estimate the ate within that world ## it returns the ate and the number of women who are in treatment require(data.table) d &lt;- data.table( group = rep(c(&#39;M&#39;, &#39;F&#39;), each = 20), po_control = c(1:20, 81:100), ## treatment_effect = 0 --&gt; sharp null is true po_treatment = c(1:20, 81:100) + treatment_effect, treatment = sample(1:0, size = 40, replace = TRUE))[ , ## notice we&#39;re now assigning outcomes := po_treatment * treatment + po_control * (1 - treatment)] ate &lt;- d[ , mean(outcomes[treatment == 1]) - mean(outcomes[treatment == 0])] n_women_treatment = d[treatment == 1 &amp; group == &#39;F&#39;, .N] return(list( data = d, ate = ate, n_women_treatment = n_women_treatment )) } "],["with-this-data-what-does-the-distribution-of-outcomes-look-like.html", "4.5 With this data, what does the distribution of outcomes look like?", " 4.5 With this data, what does the distribution of outcomes look like? experiment_one &lt;- sim_normal_study(treatment_effect = 0) experiment_two &lt;- sim_normal_study(treatment_effect = 10) experiment_one_plot &lt;- ggplot(data = experiment_one$data) + aes(x = outcomes, fill = factor(treatment), linetype = group) + geom_density(alpha = 0.5) + labs(title = &#39;No effect&#39; ) experiment_two_plot &lt;- ggplot(data = experiment_two$data) + aes(x = outcomes, fill = factor(treatment), linetype = group) + geom_density(alpha = 0.5) + labs(title = &#39;Ten unit effect&#39; ) (experiment_one_plot / experiment_two_plot) + plot_annotation(title = &#39;Measured Distribution of Estrogen, by Group&#39;) + plot_layout(guides = &#39;collect&#39;) In these two different cases – where there is no treatment effect on top, and when there is a large treatment effect on bottom – what are the group means? Where would they be on these plots? Consider the formula for the SE_{ATE}. \\[ SE(\\tau) \\approx \\sqrt{\\frac{V[\\tau]}{N}} \\] The important parts to consider for this discussion (despite being not a full statement of the SE) is that the standard error of the difference of group averages is a ratio of the underlying variance of the treatment effect, divided by the number of observations in that group. \\[ \\begin{aligned} SE[\\tau] &amp; \\approx \\sqrt{\\frac{V[Y(1)]}{n_{1}} + \\frac{V[Y(0)]}{n_{0}}} \\\\ &amp; \\approx \\sqrt{\\frac{E[\\left(Y(1) - E[Y(1)]\\right)^2]}{n_{1}} + \\frac{E[\\left(Y(0) - E[Y(0)]\\right)^2]}{n_{0}}} \\end{aligned} \\] When you examine the plot above, what are the expected values of the treatment and control groups? What does the expected value of the square of the deviations look like on this plot? "],["technical-benefits-of-blocking.html", "4.6 Technical Benefits of Blocking", " 4.6 Technical Benefits of Blocking How how does breaking this population into two smaller groups create a reduction in the calculated standard error that you observe from an experiment? What is (draw) the conditional expectation among the M group and the F group. What is (draw) the conditional variance among the M group and the F group. How has this change produced a reduction in the overall variance? experiment_one_plot &lt;- ggplot(data = experiment_one$data) + aes(x = outcomes, fill = factor(treatment), linetype = group) + geom_density(alpha = 0.5) + labs(title = &#39;No effect&#39; ) experiment_two_plot &lt;- ggplot(data = experiment_two$data) + aes(x = outcomes, fill = factor(treatment), linetype = group) + geom_density(alpha = 0.5) + labs(title = &#39;Ten unit effect&#39; ) (experiment_one_plot / experiment_two_plot) + plot_annotation(title = &#39;Measured Distribution of Estrogen, by Group&#39;) + plot_layout(guides = &#39;collect&#39;) "],["how-should-we-block-randomize.html", "4.7 How should we block randomize?", " 4.7 How should we block randomize? Let’s take several discussion points, in order: 4.7.1 What makes a useful feature? (part 1) When we are considering a block randomization to improve the power of a test, what about a feature makes it a useful blocking feature? (For instructors, probably don’t read each of these, but try to get the discussion to address them.) Does a good blocking feature have to be associated with the treatment? Does a good blocking feature have to be associated with potential outcomes? Does a good blocking feature have to have a causal effect on the measured outcomes? Suppose that have two possible features that you could use to block in the estrogen experiment. Either, you can block randomize using: blood-serum levels of estrogen, measured a week before the experiment begins; or ( “stated form” sex (i.e. female, male, nonbinary). 4.7.2 What makes a useful feature (part 2) In the async, and to this point in this live session, we have spoken only about features that are categorical for blocking. Is it possible to block on a continuous feature? What if it were measured very, very precisely, so every unit had a unique value on a continuous variable? If you could develop a method of blocking on a continuous variable, what might be the benefits? 4.7.3 Strategies of blocking If there is a benefit of creating two mini-experiments through blocking – as you have proposed in the code above – could there be a benefit to creating a third mini-experiment through blocking? What about a fourth? Is there a limit that you run into? What is the most blocks that you can produce in an experiment? Or, alternatively, what is the smallest size block that you can produce in an experiment? Is there a reason to take this strategy? What if you created many blocks, but with a noisy blocking feature. Would this work well? What if you created many blocks, but with a very precise blocking feature. Would this work well? To this point, we have discussed blocking on only a single variable. Is it possible to block on more than one variable at a time? If you have already blocked on one variable, what are the characteristics that are useful for the next variable that you consider blocking on? For example, suppose that you have already blocked the tofu experiment on experimental units’ stated-form sex. Would it be useful to then block based on wearing glasses, or hair length, or blood-serum estrogen? Why or why not? "],["clustering.html", "4.8 Clustering", " 4.8 Clustering What are the circumstances in the world that make it necessary to cluster random assign? Are these circumstances academic? Or, are there actually examples of where this might come into play? Consider the ride sharing example that we read about in Power of Experiments. What would happen if we gave some people really low prices to get into a rideshare, while we gave other people really high prices? What if they are standing next to eachother at the airport? What if one is at an airport in Oakland, while the other is at SFO? "],["blocking-or-clustering.html", "4.9 Blocking or Clustering?", " 4.9 Blocking or Clustering? 4.9.1 Let is snow! Suppose we want to measure the effect of snowplowing on local retail activity. We design an experiment that plows some locations but not others. Which of the following do you prefer? Explain the relative advantages and disadvantages of each option. On a given street, we randomly assign which businesses we plow in front of. We randomly assign which streets to plow and which streets not to plow. We randomly assign which neighborhoods to plow and which neighborhoods not to plow. Do the differences above illustrate blocking, or clustering? Returning to the snowplow example, suppose we have two wealthy neighborhoods, nine middle-class neighborhoods, and four poor neighborhoods available to experiment on. We are worried that if we put both of the wealthy neighborhoods into the treatment group, we will get an overestimate of the treatment effect of snowplowing on retail activity. We will assign treatment at the neighborhood level. Now consider blocking this experiment based on social class. Describe treatment assignment for the fifteen neighborhoods. Does blocking reduce bias? What benefit do we expect blocking to have on our ATE estimator? 4.9.2 Strolling through Berkeley David Reiley walks through Berkeley and observes retail shops. As he goes, he takes each pair of stores he encounters, flips a coin, and goes into one store in each pair to give them a free Google ad coupon. He later observes how much each spent on Google ads in the month after. Why might this increase power compared to picking stores totally at random? Reiley does the same as above, but picks one store on every street only. Reiley does the same as above, but picks two stores on every street only. Reiley picks one side of each street to treat on many streets. 4.9.3 Always low prices? Imagine that an executive at Walmart gives you the keys to the pricing at the store and asks you to determine how demand for goods changes depending on the pricing of those goods? Basically, does “rolling back prices” lead to increased demand? And by how much? What are the different levels at which you could assign different prices? What are the benefits and limitations of assigning different prices at those levels? "],["covariates-and-regression.html", "Unit 5 Covariates and Regression", " Unit 5 Covariates and Regression Adding covariates to what we’re measuring, even if those covariates are non experimental, can help us improve our measurement.” "],["learning-objectives-4.html", "5.1 Learning Objectives", " 5.1 Learning Objectives "],["covariates.html", "5.2 Covariates", " 5.2 Covariates Covariates as we will call them in this unit are are supplemental variables that do not have a causal meaning but which might predict the outcome variable. Because treatment is randomly assigned in an experiment, covariates are not required in order to generate for unbiased inference in an experiment, but including covariates in our estimation of a treatment effect might improve the precision of estimates. Typically, covariate adjustment happens through the use of a regression. Blocking (discussed last week) is doing the mechanically the same thing as regression, but blocking possesses the beneficial guarantees that all blocks will have good random assignment. One important point about covariates: For the appropriate use of covariates in an analysis of an experiment, the covariates must not change as a consequence of treatment assignment. If they change,then they are a down-stream consequence of the experiment, and therefore are a “result” of the experiment. (In the future, we will talk about why these are ‘bad controls’.) "],["rescaling-outcomes.html", "5.3 Rescaling Outcomes", " 5.3 Rescaling Outcomes Suppose that in your design, you are able to measure every unit twice, once before they are exposed to treatment, and again after they are exposed to treatment. Suppose that we have the following grammar, or notation to describe the experiment: R is a indicator for a randomization process. N is an indicator for a non-randomization process. X is an indicator that we have provided treatment to a unit. O is an indicator that we have provided control to a unit. Y is an indicator that we have made a measurement of a unit. With these operators set up, we can think about three different experiment designs. 5.3.1 Design One: Two Group Post-Test To this point, and in nearly every essay proposed for the first assignment in the class, student had in mind a two group, post-test only design. In this experiment design, we randomize an experimental population into two groups, assign treatment to one of these groups, and then observe outcomes. In many ways, the one group pre-post design is the simplest design to implement. R X Y R O Y 5.3.2 Design Two: One Group, Pre-test Post-test In this case, we take the units that are a part of our experiment, expose them to control and measure these units outcomes, and then expose them to treatment and measure these units outcomes. We might write out this design in the following way: N O Y1 X Y2 Does this meet the base definition of an experiment that you’ve written about in your homework? Would David Reiley think that this is an experiment? Would Green and Gerber think that this is an experiment? 5.3.3 Evaluate the strengths of the two designs Under what circumstances would you prefer to one or another of these two designs? Suppose that you are attempting to learn what part of your code on problem set 2 is leading to a \\(\\LaTeX\\) compile error. Which of the experiments would you propose to undertake? Suppose that you are attempting to learn the effects of giving a birthday gift to twins where measurement is magically easy. Suppose that you are attempting to learn about the effect of coffee on alertness, measured as the number of characters written down while attending async lectures. Suppose that you are attempting to learn about the effect of coffee on alertness, measured through galvanic skin conductance? Are there general principles, or circumstances that lead you to go one way or another? "],["combining-designs.html", "5.4 Combining Designs?", " 5.4 Combining Designs? By combining the two previous designs, it is possible to develop a new design that contains the benefits of each. 5.4.1 Design Three: Two Group, Pre-test Post-test In this case, we randomize into two-groups, but we also measure each unit more than once. R O Y1 X Y2 R O Y1 O Y2 This design has the benefit of the apples to apples comparison created through randomization, but additionally adds the improvement in measurement that are possible by re-scaling the outcome variable into a difference score. If we redefine the outcome to be \\(\\delta = Y_{2} - Y_{1}\\), and if there is a covariance between \\(Y_{1}\\) and \\(Y_{2}\\), which seems reasonable for many cases where the unit has “sticky” behaviors, then we are able to produce estimates of \\(\\delta\\) that are more precise because they use this “stickyness” (i.e. covariance). Even in the case when we don’t know why outcomes are correlated through time, we can still us this relationship profitably to produce estimates with smaller standard errors. "],["working-with-simple-data.html", "5.5 Working with simple data", " 5.5 Working with simple data In Field Experiments on page 74, Green and Gerber provide a table of potential outcomes for community public works projects. In the Village varaible is an index from 1-14 of the village id; in the Y variable is the outcome if assigned to control; in the D variable is the outcome if assigned to treatment; and in the Block variable is a variable that indicates the block where the unit was located. d &lt;- fread(&#39;http://hdl.handle.net/10079/cf1a6ba7-1603-4b36-ab18-1a7e81a63990&#39;) head(d) ## Village Y D Block ## 1: 1 0 0 1 ## 2: 2 1 0 1 ## 3: 3 2 1 1 ## 4: 4 4 2 1 ## 5: 5 4 0 1 ## 6: 6 6 0 1 Although this will produce numbers that are different than are reported in the book (because R implements sample variance and covariance, and the book instead uses population variance and covariance) compute the following. 5.5.1 Without blocking Compute the variance of the potential outcomes to control, the variance in the potential outcomes to treatment, and the covariance between treatment the potential outcomes to treatment and control. With these values, then, compute the standard error of the ATE. 5.5.2 With blocking Compute the variance of potential outcomes to control within each block, the variance in the potential outcomes to treatment in each block, and the covariance between the treatment and control potential outcomes. With these values, then compute the standard error of the blocked ATE. "],["using-measurements-to-diagnose-problems.html", "5.6 Using Measurements to Diagnose Problems", " 5.6 Using Measurements to Diagnose Problems "],["regression-and-multifactor-experiments.html", "Unit 6 Regression and Multifactor Experiments ", " Unit 6 Regression and Multifactor Experiments "],["learning-objectives-5.html", "6.1 Learning Objectives", " 6.1 Learning Objectives "],["good-controls.html", "6.2 Good Controls", " 6.2 Good Controls "],["bad-controls.html", "6.3 Bad Controls", " 6.3 Bad Controls What goes wrong with bad controls? Everything! "],["a-very-simple-example.html", "6.4 A Very Simple Example", " 6.4 A Very Simple Example "],["make-data-1.html", "6.5 Make Data", " 6.5 Make Data Let’s make some data in just the same way that we typically make data. We will produce a vector of potential outcoems to control, and then two outcomes that are affected by treatment. One we will consider the outcome that we are interested in understanding as a causal effect, the other, we’re going to call the “bad control”. make_data &lt;- function(n_rows=1000) { d &lt;- data.table( id = 1:n_rows, key = &#39;id&#39; ) d[ , &#39;:=&#39;( y0 = runif(min=-10, max=10, n=.N), tau = rnorm(n=.N, mean=4), D = sample(x=0:1, size=.N, replace=TRUE))][ , &#39;:=&#39;( bad_control = 1 * D + rnorm(n=.N, mean=0))][ , &#39;:=&#39;( Y = y0 + D*tau + bad_control + rnorm(n=.N) )] } d &lt;- make_data(n_rows=1000) "],["what-is-the-causal-model-we-hold.html", "6.6 What is the causal model we hold?", " 6.6 What is the causal model we hold? When we are thinking about the causal model here, we’re saying, “I think that the conditional expectation of Y depends on the treatment status”. But, maybe I think I want to also control for the variable bad_control. In fact, we can estimate a reliable causal effect for either D on Y, or D on bad_control, but not the two together. model_1 &lt;- d[ , lm(Y ~ D)] model_2 &lt;- d[ , lm(Y ~ bad_control)] stargazer( model_1, model_2, type = &#39;text&#39;, se = list(rse(model_1), rse(model_2)), omit.stat = c(&#39;ser&#39;, &#39;f&#39;) ) ## ## ========================================= ## Dependent variable: ## ---------------------------- ## Y ## (1) (2) ## ----------------------------------------- ## D 5.154*** ## (0.391) ## ## bad_control 1.821*** ## (0.175) ## ## Constant 0.201 1.704*** ## (0.266) (0.218) ## ## ----------------------------------------- ## Observations 1,000 1,000 ## R2 0.149 0.095 ## Adjusted R2 0.148 0.094 ## ========================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 6.6.1 Do the estimates match the world? When you look at what the models have estimated, do they match the data that we created above? model_3 &lt;- d[ , lm(Y ~ D + bad_control)] stargazer( model_1, model_2, model_3, type = &#39;text&#39;, se = list(rse(model_1), rse(model_2), rse(model_3)) ) ## ## ============================================================================================== ## Dependent variable: ## -------------------------------------------------------------------------- ## Y ## (1) (2) (3) ## ---------------------------------------------------------------------------------------------- ## D 5.154*** 4.166*** ## (0.391) (0.428) ## ## bad_control 1.821*** 1.069*** ## (0.175) (0.188) ## ## Constant 0.201 1.704*** 0.122 ## (0.266) (0.218) (0.263) ## ## ---------------------------------------------------------------------------------------------- ## Observations 1,000 1,000 1,000 ## R2 0.149 0.095 0.176 ## Adjusted R2 0.148 0.094 0.175 ## Residual Std. Error 6.157 (df = 998) 6.349 (df = 998) 6.061 (df = 997) ## F Statistic 174.636*** (df = 1; 998) 105.053*** (df = 1; 998) 106.678*** (df = 2; 997) ## ============================================================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Now we’re left with an under-estiamte of the causal effect of D on Y, and we’ve got some estimate of the effect of bc on Y. But, when we built the data, there wasn’t such an effect! In fact, what is in here is the relationship between y0 and Y, but through a mangled causal pipeline. "],["a-more-complicated-example.html", "6.7 A More Complicated Example", " 6.7 A More Complicated Example # 1. create data.table # 2. create potential outcomes to control # 3. create a randomly assigned treatment indicator # 4. have that treatment indicator and potential outcomes to control # cause outcomes in B. # 5. dichotomize B (think of it like &quot;finishing college&quot; in MHE) # 6. build the potential outcomes to treatment as the joint effects of # a. potential outcomes to control # b. the effect of B (which was caused by treatment) # c. the direct effect of treatment on the outcome variable # 7 make an &quot;observed Y&quot; vector, just like usual. n_rows &lt;- 100 d &lt;- data.table(id = 1:n_rows) d[ , y0 := sample(-10:10, n_rows, replace = T)] d[ , treat := sample(c(0,1), n_rows, replace = TRUE)] d[ , B := y0 + 5 * treat + rnorm(n_rows, mean= 0, sd = 4)] d[ , B := B &gt; 0] d[ , y1 := y0 + B + 10 * treat + round(rnorm(n_rows, mean= 0, sd = 4), 0)] d[ , Y := y1*treat + y0*(1-treat)] "],["look-at-data.html", "6.8 Look at Data", " 6.8 Look at Data hist(d[,Y], col = &quot;grey&quot;, main = &quot;Histogram of Observed Outcomes&quot;) "],["estimate-relationships.html", "6.9 Estimate Relationships", " 6.9 Estimate Relationships What are the estimates we might be interested in? 6.9.1 Correct Well, the first is the correct relationship which is the causal effect of treat on Y. How would we estimate this? Since the underlying conditional expectation function tells us something causal, we can just use a simple linear regression as a method of estimating the causal effect. So, of course, just with a simple linear model. m1 &lt;- glm(Y ~ treat, data = d, family = &quot;gaussian&quot;) stargazer(m1, type = &quot;latex&quot;, omit.stat = &quot;f&quot;, header = FALSE) 6.9.2 Incorrect The second, incorrect relationship might look at the effect of treatment, among people who have different levels of B. This would just be looking at a model that has both features built into it. Another, simliarly bad estimate might be to subset the model into different groups and look for the treatmnet effect within these groups. This also is silly and, as we’ll demonstrate, will not recover anything even remotely related to the treatment effect we’re interested in. m2 &lt;- glm(Y ~ treat + B, data = d, family = &quot;gaussian&quot;) m3a &lt;- glm(Y ~ treat, data = d[B==TRUE], family = &quot;gaussian&quot;) m3b &lt;- glm(Y ~ treat, data = d[B==FALSE], family = &quot;gaussian&quot;) stargazer(m1, m2, m3a, m3b, omit.stat = &quot;f&quot;, header = FALSE, add.lines = list(c(&quot;Subset B?&quot;, &quot;All&quot;, &quot;All&quot;, &quot;T&quot;, &quot;F&quot;)) ) "],["robust-standard-errors.html", "6.10 Robust Standard Errors", " 6.10 Robust Standard Errors David R. makes the good point in the async material that if we don’t have a good reason to assume that the variance is the same between different groups, or really across all values of our explanatory variables, then these variances might, in fact be different, and as a consequence we might have overly optimistic estimates of our standard errors. Why would this be bad? As we’ve said in the past, if we only want to falsely reject the null hypothesis in 5% of cases due just to chance (roughly an equivalent thought to a 95% confidence interval), then if our standard errors are wrong, there is the possibility that we falsely reject the null more frequently. So, we think we’re only making this type of mistake in 5% of cases to to random chance, but perhaps we’re actually making this type of mistake in 20% of cases. Why would this be bad? Remind yourself? Luckily, it is pretty easy to estimate robust standard errors. In fact, acknowledging heteroskedasticiy does not have ANY effect on the location of our estimates of the relationships between variables. What does this mean? It means that the estimated \\(\\beta_{1}\\) that you pull off of some regression is the same whether you are using homoskedastic or heteroskedastic-consistent standard errors. What is actually happening when we compute HCE? Well, rather than presuming that all the residuals are the same, instead we’re actually calculating those residuals from from the regression line. What is the penalty we pay for this? Well, in the case of homoskedastic error, we have a slightly less efficient estimator (which makes our findings more conservative when they don’t need to be). And because we’re estimating things, we’re burning a few degrees of freedom. Otherwise though, there isn’t really that strong a penalty to pay. library(sandwich) # estimates HCE easily library(lmtest) # sets up t-test easily d &lt;- data(petersen) head(d) m1 &lt;- lm(y ~ x, data = d) ## since i have the lmtest loaded; i can call: coeftest(m1, vcov = vcov(m1)) ## to estimate robust SEs is a two line solution m1$r.vcov &lt;- vcovHC(m1) coeftest(m1, vcov = m1$r.vcov) These two packagse are recommended packages and are very standard in R. data.table which I’ve been harping on is a big deal. Lots of people use it. So too is ggplot2. But these two packages, sandwich and lmtest, are core. There is no disputing that. m2$r.vcov &lt;- vcovHC(m2, type = &quot;HC3&quot;) coeftest(m2, vcovHC(m2, type = &quot;const&quot;)) coeftest(m2, m2$r.vcov) What has happened in these last two calls? In the first, we estimated the HCE structure using the vcovHC function. In the second, we use that vcov in a t-test for each of the coefficients. There is a specific relationship between the variance-covariance matrix and the standard error. in fact, it is very much like the relationship beween the variance and standard error in any other application we’ve examined so far. This relationship is the following: \\[ SE(\\hat{\\beta}) = \\sqrt{diag(vcov)} \\] So, all we’re really doing is making a post-estimation correction to the variance covariance matrix, and then dividing by this new standard error. Quite straightforward. Why would you want to know this little bit? If you’re going to run the test yourself, you will want to be able to pull off the SEs from the vcovHC object. t.numerator &lt;- coef(m2) t.denominator &lt;- sqrt(diag(vcov(m2))) t.denominator.robust &lt;- sqrt(diag(vcovHC(m2, type = &quot;HC1&quot;))) # t.ratios: # not robust: t.numerator / t.denominator # robust t.numerator / t.denominator.robust But, like as I showed earlier, we can wrap all this up with the lmtest package’s call coeftest. coeftest(m2, vcov(m2)) coeftest(m2, vcovHC(m2)) What if we wanted to pretty-print ourselves a table? If we are using stargazer, or other packages, we will need the SEs off that model. m2.se &lt;- sqrt(diag(vcov(m2))) m2.rse &lt;- sqrt(diag(vcovHC(m2, type = &quot;HC1&quot;))) stargazer(m2, m2, se = list(m2.se, m2.rse), type = &quot;latex&quot;, header = FALSE) "],["what-about-clustered-standard-errors.html", "6.11 What about clustered standard errors?", " 6.11 What about clustered standard errors? Ok, now we’re a little deeper down the rabbit hole. As we’ve talked about, clustered standard errors acknowledge that you’ve got treatment assigned at the cluster level, and that there may be significant covariance in potential outcomes at that cluster level. If this is the case, then we have functionally fewer observations than we have nominally, and we also have less power to detect an effect. We’ve provided you a function to calculate the clustered standard errrors in the homework. And, we’ve had some considerably back-and-forth between two people who are named Alex on slack. Another, relatively recently developed package now exists that shoud make considerably more simple the task of estimating clustered standard errors. The package has been developed since the recording of the async material, so we’ll do the work to present it here. The package is the multiwayvcov. data(&quot;PetersenCL&quot;, package = &quot;sandwich&quot;) m1 &lt;- lm(y ~ x, data = PetersenCL) summary(m1) ## ## Call: ## lm(formula = y ~ x, data = PetersenCL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7611 -1.3680 -0.0166 1.3387 8.6779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02968 0.02836 1.047 0.295 ## x 1.03483 0.02858 36.204 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.005 on 4998 degrees of freedom ## Multiple R-squared: 0.2078, Adjusted R-squared: 0.2076 ## F-statistic: 1311 on 1 and 4998 DF, p-value: &lt; 2.2e-16 ## when we clusetr coeftest(m1, vcovCL(m1, ~ firm)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.029680 0.067013 0.4429 0.6579 ## x 1.034833 0.050596 20.4530 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## when we don&#39;t cluster coeftest(m1, vcov(m1)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.029680 0.028359 1.0466 0.2954 ## x 1.034833 0.028583 36.2041 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Cluster by both firm, and year coeftest(m1, vcovCL(m1, ~ firm + year)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.029680 0.065064 0.4562 0.6483 ## x 1.034833 0.053558 19.3217 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Pretty print that. stargazer(m1, m1, m1, se = list(sqrt(diag(vcov(m1))), sqrt(diag(vcovCL(m1, ~ firm))), sqrt(diag(vcovCL(m1, ~ firm + year)))), type = &#39;text&#39;, header = FALSE) ## ## ====================================================================== ## Dependent variable: ## -------------------------------------- ## y ## (1) (2) (3) ## ---------------------------------------------------------------------- ## x 1.035*** 1.035*** 1.035*** ## (0.029) (0.051) (0.054) ## ## Constant 0.030 0.030 0.030 ## (0.028) (0.067) (0.065) ## ## ---------------------------------------------------------------------- ## Observations 5,000 5,000 5,000 ## R2 0.208 0.208 0.208 ## Adjusted R2 0.208 0.208 0.208 ## Residual Std. Error (df = 4998) 2.005 2.005 2.005 ## F Statistic (df = 1; 4998) 1,310.740*** 1,310.740*** 1,310.740*** ## ====================================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 "],["heterogeneous-treatment-effects.html", "Unit 7 Heterogeneous Treatment Effects", " Unit 7 Heterogeneous Treatment Effects When we consider heterogeneous treatment effects, we acknowledge that it is very unlikely that every single person reacts to treatment in exactly the same way. But, are there certain types of people who always react more strongly to treatment? Are there certain types of people who always react less strongly? How should we go about (a) looking for HTEs; (b) testings for HTEs with nominal p-value coverages; and (c) reporting HTEs to individuals in a way that make sense? "],["learning-objectives-6.html", "7.1 Learning Objectives", " 7.1 Learning Objectives At the end of this week, students will be able to Understand what an HTE is, and what it is not. Conduct, test, and interpret models with interaction terms as specific tests of hypotheses. "],["reading-and-discussion-goodson.html", "7.2 Reading and Discussion: Goodson", " 7.2 Reading and Discussion: Goodson Why do we call them A/B tests, rather than experiments? Are you uncomfortable with the idea that companies are experimenting on you? Are you uncomfortable experimenting in your subjects? If there is a gap between your feeling about being experimented on compared to how you feel when you are doing the experimenting, why does this gap exist? What is an A/B test in Goodson’s estimation? How should we know when a test should be determined to be complete? How should we determine that one experience is causing different behaviors in our reference population than another experience? Can we decide this while we are working through the experiment? Can we make this choice when we see the outcome that we thought we were going to see? Can we be sure that we have set the correct stopping rules ahead of time? What are the consequences of peaking, and then stopping early, once we have seen the results? "],["coding-and-demo-the-californians.html", "7.3 Coding and Demo: The Californians", " 7.3 Coding and Demo: The Californians library(data.table) library(stargazer) makeData &lt;- TRUE set.seed(1) if(makeData) { sim_size &lt;- 1000 d &lt;- data.table(californian = sample(c(0,1), sim_size, replace = TRUE), affluence = sample(1:7, sim_size, replace = TRUE), preY = rnorm(sim_size, mean = 100, sd = 7), treat = sample(c(0,1), sim_size, replace = TRUE) ) d[ , literacy := rnorm(sim_size, mean = d$affluence, sd = 2)] d[ , tau := rnorm(sim_size, mean = 5 + californian)] ## + d$affluence) d[ , postY := preY + tau*treat] } ## What is the (unobserved) true average treatment effect? ## Estimate a model that includes only the treatment effect. ## Interpret all the coefficients. mod0 &lt;- ## Estimate a model that includes the treatment effect and ## the (highly predictive!) pre-treatment Y val. mod1 &lt;- ## Print the two next to each other, and compare what is going on. ## Tell me what is happening in the differences in the intercept ## and the preY coefficients. Tell me what is happening in the SE ## for the treatment effect. stargazer(mod0, mod1, type = &quot;text&quot;) ## Subset the data into two groups based on gender and estimate ## a model that only includes the preY and treatment effects. ## Print these two models side by side, and tell me what is going ## on. mod2 &lt;- mod3 &lt;- stargazer() ## Based on this, would you conclude that these are different? ## Talk about the 95% CI for each. ## ?confint ## Now, estimate two more models: ## 1. A model with preY, treatment and gender indicator ## 2. A model with prey, treatment, gender, and treatment*gender ## interaction ## 3. Test for the necessisity of the interaction; first using a ## t-test. This should be really, really simple. Then, using a ## f-test for the nested models (see the anova(...) call). ## Is the p-value the same or different for this test? ## Why? mod4 &lt;- mod5 &lt;- ## ? anova ## Finally, use the results from model 5 to tell me what the treatment ## effect is for males and for californians. ## ## AT HOME: ## Work to examine what including the other affluence and literacy ## triggers does to your estimates. ## "],["coding-and-discussion-tips-at-a-restaurant.html", "7.4 Coding and Discussion: Tips at a Restaurant", " 7.4 Coding and Discussion: Tips at a Restaurant ## Green and Gerber: Question 9.6 ## a, b, and c. d &lt;- fread(&#39;http://hdl.handle.net/10079/cd6be01a-a827-4312-a2fa-74329ce7f96d&#39;) ## a. (Probably skip this one) ## Suppose that you ignored the gender of the server and simply analyzed whether ## the happyface treatment has and effect (and/or) a heterogeneous effects. Use randomization inference ## to test whether the Variance of \\tau = 0 using randomization inference by ## comparing the variance of potential outcomes in treatment and control. ## b. Write down a regression model that depicts the effect of the gender of ## the waitstaff, whether they put a happyface on the bill, and the interaction ## of these factors. ## ## c. Estimate the regression model that you wrote down in (b) and test the ## interaction between waitstaff and the happyface treatment. ## Is the interaction significant. ## d. Waiting tables in the time of covid: Suppose that you&#39;re on the waitstaff ## at this restaurant, and while you&#39;re waiting tables you&#39;re FULLY garbed ## up: facemask, face-shield, full operating gown, and so on. ## What this means is that you have the choice to reveal a gender identity ## that is either &quot;Male&quot; or &quot;Female&quot; to the patrons. ## ## - Is there one gender identity that receives higher tips in this restaurant? ## - Is there a gender that has a higher treatment effect? What is the ## test that you would run to assess this? "],["treatment-noncompliance.html", "Unit 8 Treatment Noncompliance", " Unit 8 Treatment Noncompliance This begins a section of the class where we are going to evaluate what happens when problems creep into the actual experiments that we are conducting. We are first going to look at what happens when we instruct people to take treatment, but they choose not to. Or, when we instruct people to take control, but they choose to take treatment instead. It might seem, at first, like we should just proceed by analyzing according to the treatment condition that they actually received. However, because we haven’t experimentally assigned this condition, this creates an unprincipled estimator. This doesn’t mean that all is lost however. We can redefine the causal quantity that we are estimating, and produce a reliable estimate of this new concept.We’re going to present two such concepts this week. The first concept is the idea of the intent to treat effect (the ITT). The second concept is the idea of the treatment effect among compliers, which we will call the CACE. "],["learning-objectives-7.html", "8.1 Learning Objectives", " 8.1 Learning Objectives Recognize when experimental units have not complied with the treatment assignments they were given, and appreciate that this causes problems for our two-group estimator. Recover causal estimators, but for sub-populations of the overall population. Utilize a new class of model, the two-stage least squares model, or 2SLS, which is the appropriate model choice when we are dealing with either one- or two-sided non-compliance. "],["starting-conversation.html", "8.2 Starting conversation", " 8.2 Starting conversation Life on campus is exciting! Whether students are involved in affinity groups, advocacy groups, protest groups, or just party groups, student life on campus is exciting. We’re not to be left out! We’re not to be denied the chance to make our voices heard. But, because we’re usually calling in via Zoom, we don’t have the typical problems that students and faculty complain about – parking, enrollment, sports tickets, and beer availability. What we do have to complain about is that “god awful” sound of the bell-tower ringing every hour on the hour. Suppose that we are to discuss this very. important. issue. before a panel of the deans and University administration. And, further suppose that in light of global events of the past several years, they’re actually amenable to what we’re proposing – cutting off those bells, and providing the Berkeley Carillon player a generous retirement. University Carillonist However, there’s a catch. They are worried that taking such an action would be detrimental to the student experience on campus, and they would like to measure the causal effect of playing vs. not playing the Carillon while students are changing classes. In breakout rooms, please design an experiment that would be able to measure the difference in student experience. You will have to propose a measurement, a timescale for that measurement, and a feasible randomization that could actually occur given the real-world constraints that what is at question are loud sounds emanating from a 300 foot tower in the middle of a large, busy campus. If there are any limitations to what you design, please voice those concerns and talk about why they arise, relative to an ideal experiment (that you are proably unable to conduct). "],["design-notation.html", "8.3 Design Notation", " 8.3 Design Notation This week, you read three very short chapters in a book by Trochim and Donelly. This reading begins with a series of one-group “threats” to causal inference, which we will enumerate again here: History threat Maturation threat Testing threat Instrumentation threat Mortality threat Regression threat Many of these contain a plain language statement of a problem that might arise from an experiment design. For example, a maturation threat might mean that as your subjects get older or more experienced through the experiment, they may do better (or worse) at the task that they are being asked to undertake. This isn’t an academic-only concern, this is something that is actually likely to happen if you measure performance over a long period of time. The author then moves on to describe several multiple-group threats. Notice that each of these multi-group threats are simply “selection-” version of the threads that we have already enumerated. How to do we ensure that we do not witness any of the problems created by these selection- threats? 8.3.1 Design Notation Finally, the authors introduce us to the real point of this week: design notation whereby they provide us with a constrained set of actions that can be taken. R O Y N X "],["non-compliance-discussion.html", "8.4 Non-compliance Discussion", " 8.4 Non-compliance Discussion What is ITT? What is ITT_{d}? What is the CACE? How does this produce an apples to oranges comparison? What is the exclusion restriction, and why is it important in this case? "],["estimating-with-non-compliance.html", "8.5 Estimating with Non-compliance", " 8.5 Estimating with Non-compliance 8.5.1 Estimating with non-compliance library(data.table) library(magrittr) library(lmtest) # though we won&#39;t actually use them in this library(sandwich) # and neither this... ## install.packages(&quot;AER&quot;) # this has a nice wrapper for iv regression # but we can do it by hand with VERY little work nrows = 1000 d &lt;- data.table(id = 1:nrows) d[ , y0 := rnorm(nrows, mean = 10)] d[ , tau := rnorm(nrows, mean = 5)] d[ , y1 := y0 + tau] d[ , assigned := sample(rep(c(0,1), each = nrows / 2))] # z in the book d[ , treated := 0] d[assigned == 1, treated := sample(c(1,0), size = .N, replace = TRUE, prob = c(.7, .3))] d[treated == 1, Y := y1] d[treated == 0, Y := y0] d[ , mean(y1 - y0)] ## [1] 5.023489 But, that has all the data in the science table. In real life, we won’t get this. d2 &lt;- d[ , .(assigned, treated, Y)] d2[ , .(mean = mean(Y)), by = assigned][ , diff(mean)] ## [1] 3.427019 5 * .7 ## [1] 3.5 ## rats. we /know/ that the treatment effect is 5, but when we look at the ## intent to treat effect, we only estimate a difference of 3.5 or so. Suppose that we cannot evaluate whether someone was /actually/ treated or not. - In this case, we will ONLY be able to recover the ITT. - This requires that you suspect reality for a moment and suppose we don’t have the treated measurement. 8.5.2 Estimate the Intent to Treat Effect 8.5.3 Estimate the Compliance Rate 8.5.4 Compute the CACE Because you have the ITT and the compliance rate, estimate the CACE 8.5.5 Do you get the same thing with this subset estimator? d2[assigned == 1 , .(group_mean = mean(Y)), by = .(treated)] %&gt;% .[ , diff(group_mean)] ## [1] -5.104661 Notice that this does not have clear estimate of the uncertainty associated with it. This is a bummer. In order to estimate with a reliable standard error, we can turn to two stage least squares. "],["two-stage-least-squares.html", "8.6 Two Stage Least Squares", " 8.6 Two Stage Least Squares Two-stage least squares estimators have the benefits of Doing exactly the same thing that the CACE = ITT / ITT_{d}; but, Doing it in a way that has known standard errors that are quickly and easily computable. 8.6.1 First Stage In the first stage we: Estimate the proportion of people who are receive treatment as a function of being assigned to treatment. In the case of one-sided non compliance this is exactly the same thing as estimating the ITT_{d}, right? first &lt;- &#39;fill this in&#39; ## calculate the fitted values from this regression ## - that is, just multiply the coefficients that you estimate from the ## first stage times the data values. In the event that the exclusion ## restriction holds, then these predictions are just orthagonal to every ## thing that is not modeled in your data! d2[ , predict := predict(first)] 8.6.2 Second stage In the scond stage we: Estimate the relationship between the predicted values and the outcome. This will just tell you how the outcome changes in response to the amount of change that your treatment assignment is able to produce. second &lt;- &#39;fill this in&#39; # coeftest(second, vcovHC(second, type = &quot;const&quot;)) ## these ses might be wrong. ## I&#39;ll note that the standard errors from this &quot;hand-rolled&quot; 2SLS will not ## be correct (due to some accounting issues in the variance between the predictions ## in the first stage and the second stage. ## ## we can fix this by hand -- though I wouldn&#39;t -- or we can use a library that will ## do the accounting for us, from the library AER library(AER) ## Loading required package: car ## Loading required package: carData ## Loading required package: survival iv.model &lt;- ivreg(Y ~ treated | assigned, data = d2) coeftest(iv.model, vcov = vcovHC(iv.model, type = &quot;const&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.034882 0.050138 200.146 &lt; 2.2e-16 *** ## treated 4.923877 0.101876 48.332 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["spillover-and-interference.html", "Unit 9 Spillover and Interference", " Unit 9 Spillover and Interference At the outset of the course, we enumerate three hard-core requirement of an experiment design. In addition to intervening in the world, to produce an unbiased estimator of a treatment effect, we require that an experiment: Assign that intervention to experimental units at random to eliminate the possibility of confounding due to selection bias; That one, and only one difference exists between two comparison groups, thereby allowing us to exclude all other possible causes but for the feature that we have experimentally assigned; and, That the treatment experienced by one experimental unit does not “interfere” with the potential outcomes of another unit. In previous weeks, we’ve engaged with how to evaluate whether a treatment has been successfully randomized. In this week’s materials, we are going to examine what, if anything, we may do in response to interference between units. There are two possibilities. First, we might design our experiment to minimize the effects of interference between units by re-designing or measuring differently. In doing so, we endeavor to maintain the measurement of an individual-level treatment effect, through a multi-group experiment. Second, we might acknowledge the existence of interference and expand our thinking about what is a treatment effect. "],["learning-objectives-8.html", "9.1 Learning Objectives", " 9.1 Learning Objectives At the conclusion of this week, student will be able to: Articulate in clear terms what circumstances are, and what circumstances are not interference events. Appreciate, and evaluate the extent that interference between units changes both the concept of a treatment effect, and also how a multi-group measurement’s estimates change in response to interference between units. Identify common situations where interference is likely to occur, and anticipate some methods of mitigating, ameliorating, or designing in response to this interference. "],["defining-terms.html", "9.2 Defining Terms", " 9.2 Defining Terms What does it mean for one unit to interfere with another unit? If two units communicate with one another, is this interference? If three units are all genetically related to one another, is this interference? If ten units all work in the same building, is this interference? If two partners share a tablet for browsing the internet, is this interference? Now, be very precise with your language: Using the term “potential-outcomes” how do Green and Gerber define interference? "],["defining-notation.html", "9.3 Defining Notation", " 9.3 Defining Notation 9.3.1 Identify concepts Until this week, we have used two concepts to describe treatment assignment and application: \\(Z\\) is the assignment to treatment; and, \\(D\\) is the dose received of treatment. What concept is identified in the following notation: \\(E[Y_{i}(1) | D_{i} = 1]\\)? Is this measurable? \\(E[Y_{i}(1)]\\) Interpret the expression \\(Y_{i}(\\mathbf{d}) = Y_{i}(d)\\) and explain how it conveys the non-interference assumption. 9.3.2 Classroom Assignments (From Green and Gerber, p. 283): Sometimes researcher are reluctant to randomly assign individual students in elementary classrooms because they are concerned that treatments administered to some students are likely to spill over to untreated students in the same classroom. In an attempt to get around possible violations of the non-interference assumption, they assign classrooms as clusters to treatment and control, and administer the treatment to all students in a classroom. State the interference event that commonly leads researcher to assign an entire classroom to a condition. State the interference assumption that is implicitly made when classrooms are cluster random assigned. Where, if anywhere does the researcher assume that spillover exists? Where, if anywhere, does the researcher assume that spillover not exist? An estimand is the concept that an estimator is attempting to estimate. For example, the ATE estimator produces an unbiased, consistent estimate of the individual-level causal effect. What causal estimand does the clustered design identify? Does this estimand include or exclude spillovers within classrooms? What about spillovers between classrooms? What about spillovers between schools? 9.3.3 Working with a simple example Suppose that we are conducting an experiment where we examine the effects of releasing solution sets early to some students in the 241 classroom. What form of interference is possible? Suppose that Abby, Bobby, Cathy and David are all on a project team together. Furthermore, suppose that all members of the team work well together, have an ambitious class project that they are working on, and talk regularly. If every one of the students were to be assigned to the control group, name values that are plausible for their completion time on problem set three. Suppose that Abby, Bobby and Cathy are assigned to the control group, but that David is assigned to the treatment group. What do you think will happen in their daily project meeting? Suppose that, no matter the empirical reality, you assume that there is no interference within this group. What would you call the value that you observe for Abby, given this assumption? Consistent with what you have said will happen in their daily project meeting, what values are you actually seeing for Abby? What are the consequences for your estimated treatment effect? Suppose that Abby and Bobby are assigned to the control group and that Cathy and David are assigned to the treatment group. What do you think will happen in their daily project meeting? What would you call the value that you observe for Abby, given this assumption? Is it different when both Cathy and David are assigned to treatment compared to when only David is assigned to treatment? Given what you have stated about this small world, how many treatment assignment conditions do you have to be aware of? 9.3.4 Working with a more complex example Suppose that we are conducting an experiment where we examine the effects of releasing solution sets early to some students in a law school classroom. Law school is notoriously competitive, and outside one’s immediate group of friends, there is little collaboration. Suppose that A, B, C and D are again friends. Suppose that W, X, Y and Z are also friends. But suppose that the two groups are not friends between groups. If A receives an exam solution, but W, X, Y and Z do not, what would you call the values observed for W, X, Y and Z? If A, B, and C receive an exam solution, but D, W, X, Y and Z do not, what would you call the values observed for D, W, X, Y and Z? Given this example, are the potential outcomes for un-curved exam score different for A if W receives or does not receive a solution? Given this example, are the potential outcomes for curved exam score different for A if W receives or does not receive a solution? "],["within-subjects-experiments.html", "9.4 Within subjects experiments", " 9.4 Within subjects experiments Earlier in the course, we talked about two-group pre-test/post-test experiments. These experiments are exceptionally strong against a large series of threats to identification. And, they form the basis of the expanded topic of a within-subject experiment. What is a within subject experiment? When might you propose that a within subject experiment would be advisable? Why? What is the benefit of a within subject experiment? When are within subject experiments difficult to conduct? Green and Gerber, and the async identify two requirements of within-subjects experiments: No anticipation No persistence What do these two assumptions mean in terms of what you are measuring at the individual-time level? Suppose that you were worried that your experimental units might either anticipate being put into treatment or that the treatment they take might have long-run effects. How might you design a test to see if either of these concerns are present in your design? 9.4.1 Survey Experiments (From Green and Gerber, p.285): Concerns about interference between units sometimes arise in survey experiments. For example surveys sometimes administer a series of vignettes involving people with different attributes. A respondent might be told about a low-income person who is randomly described as white or black; after hearing the description, the respondent is asked to rate whether this person deserves public assistance. The respondent is the presented with a vignette about a second person, again randomly described as white or Black, and asked about whether this person deserves public assistance. This design creates four experiment groups: Two vignettes describing Black beneficiaries; Two vignettes describing white beneficiaries; A vignette describing a Black beneficiary first, followed by a white beneficiary; and, A vignette describing a white beneficiary first, followed by a Black beneficiary. Suppose that each respondent provides a rating after each vignette. Questions to answer: Propose a model of potential outcomes that reflects the ways that subjects might respond to the treatment and the sequences in which they are presented. How might you represent this using the R O X Y grammar? Using your model of potential outcomes, define all of the ATE or ATEs that a researcher might seek to estimate. Suggest an experiment design that could estimate this/these causal estimand(s) using observed data. Suppose a researcher analyzing this experiment estimates the average race effect by comparing the average evaluation of the white recipient to the average evaluation of the black recipient. Is this a sound approach? Why or why not? "],["discussing-the-reading-blake-and-coey-2014.html", "9.5 Discussing the reading: Blake and Coey (2014)", " 9.5 Discussing the reading: Blake and Coey (2014) Here is a link to the reading. What is the treatment, and how does treatment assignment work? What is the outcome, and how is it measured? How does this experimental setup generate spillovers within an auction? What is the naive research strategy that produces a biased estimate in the presence of the spillover? Tell a story to explain why the within-auction spillovers might give you upward bias in the measured treatment effect. (Optional; harder) How does the experiment generate spillovers between auctions? Tell a story to explain why you might get downward bias from between-auction spillovers. What is the proposed empirical analysis strategy to reduce the bias? What would be a better experimental design to conduct in the first place? Do you see an example of a stepped-wedge design in this article? Explain. "],["discussing-the-reading-miguel-and-kremer-2004.html", "9.6 Discussing the reading: Miguel and Kremer (2004)", " 9.6 Discussing the reading: Miguel and Kremer (2004) Here is a link to the reading. What question are Miguel and Kremer trying to answer? Why is this important? What is the spillover problem in this setting? How did doctors get the wrong answer in randomized trials before Miguel and Kremer addressed the spillover problem? (The article refers to this as a double penalty.) When not taken into account correctly, did the spillovers to cause underestimation or overestimation of the treatment effect? Explain why. Which feature do the authors choose to make their experiment less vulnerable to this spillover problem? How do the authors still have a (smaller) spillover problem despite this design decision? What was the compliance rate for those whom the researchers intended to treat in 1998? Name two kinds of noncompliance described in the article, and say which one was largest. Due to noncompliance, we can only measure the CACE rather than the ATE. Why is the CACE just fine for the policy question asked in the article? Do you see an example of a stepped-wedge design in this article? Explain. "],["causality-from-observational-data.html", "Unit 10 Causality from Observational Data", " Unit 10 Causality from Observational Data punkin belly What happens if we cannot run an experiment? Perhaps we don’t have the budget or time, perhaps the context is too fraught to conduct an experiment. Should we walk away and learn nothing? "],["learning-objectives-9.html", "10.1 Learning Objectives", " 10.1 Learning Objectives At the end of this weeks extensive content, students should be able to Describe a series of techniques that have been proposed to estimate causal effects even when a randomized experiment has not been conducted; Evaluate whether a particular technique matches with the data generating context; Analyze whether an observational data technique is likely to identify a treatment effect; and, Communicate the risks and limitations that are brought about when using observational data to make causal claims. "],["the-experimental-ideal.html", "10.2 The Experimental Ideal", " 10.2 The Experimental Ideal If you’ve been through this once, you’ve been through it one-hundred times this semester, but it might be worth re-stating what we get out of conducting a randomized experiment. Consider this a sage smudging at the beginning of a spooky week. Why do we conduct experiments? What guarantees exist as a result of a well-run experiment? "],["a-continuum-of-plausibility.html", "10.3 A Continuum of Plausibility", " 10.3 A Continuum of Plausibility As we are talking today, consider the fully-randomized, full-compliance, full-reporting, high-powered field experiment to be the high-water mark of credibility. Under such a scenario, we can think of any analysis that we undertake as producing a highly-credible, highly-reliable estimate of a treatment effect. Through our discussion this week, we hope to name where we think other techniques and data generating processes fall relative to this high-water mark. Some, as we will see, might actually produce estimates that are very nearly as credible as the experimental ideal. Others are ghastly in their performance. However, as data scientists who have to get work done we need to be able to produce the best possible statement about a treatment effect, and if we have any misgivings about those statements, be able to provide a clear statement about the risk that is attendant to using them. "],["natural-experiments.html", "10.4 Natural Experiments", " 10.4 Natural Experiments Natural experiments are experiments that have been conducted by someone other than the researcher. If you remember back to Problem Set 1, consider the case of the early childhood education that is provided by the state. When the state chose who to provide education to based on need, this was clearly not an experiment because it isn’t possible to fully understand the selection criteria used by the state, and so it is not possible to make a strong statement that any estimate produced from a two-group estimator wouldn’t be possibly subject to confounding. But, what about the case where the state randomly assigned some kids to get the treatment? Is there any reason that we should discount this simply because it wasn’t us to do the assigning? What hubris! 10.4.1 Questions to consider What are the hallmarks of a natural experiment? How would you propose to structure your search for natural experiments? How will you know when you’ve actually found something that is a natural experiment? 10.4.2 Breakout activity What are the things in the past year of your lives that have seemed to arrive at random? How would you know if they actually are at random? After the members of the team have spent a few moments thinking about things that might be random, ask yourselves, “What might we be able to learn downstream from this experiment?” What is the most plausible thing that you might learn? What is the longest, most extreme possibility that you might learn? 10.4.3 How does one analyze a natural experiment? If a natural experiment is just a randomization conducted by someone else – is there anything different that we need to do in order to analyze it? Why or why not? We talk, with some specificity this week, about estimating using two-stage least squares regression. What is this technique, what does it promise to us, and how does it work? Consider simulated data that is created in the following way: ability, family_income, and lottery winning to get into a “magnet” school are all random However, suppose that schooling which is the indicator that someone actually got schooling at a magnet school is correlated with ability, with family income, and with lottery. What would be the consequence of estimating an eventual outcome, using a naive regression? ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.107807 0.089085 337.967 &lt; 2.2e-16 *** ## schooling 2.037821 0.075628 26.945 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How close, or far from the truth is this estimate? How sure are you that this is different from zero? What relationship would you have to change in this data generating process in order to flip the bias of the estimate from estimating a value that is higher than the truth, to estimate a value that is lower than the truth? "],["can-we-fix-this-estimate.html", "10.5 Can we fix this estimate?", " 10.5 Can we fix this estimate? The promise of two stage-least squares is that it produces unbiased estimates so long as we’re able to find something that is random. first_stage &lt;- d[ , lm(schooling ~ lottery)] d[ , schooling_hat:= predict(first_stage)] second_stage &lt;- d[ , lm(Y ~ schooling_hat)] coeftest(second_stage, vcov. = vcovHC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.14611 0.15159 198.8665 &lt; 2.2e-16 *** ## schooling_hat 1.96865 0.20180 9.7557 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How or why does this work? How does simply making predictions from the first stage regression generate eventual estimates that are unbaised? Consider looking at the residuals from the first stage regression d[ , residuals := resid(first_stage)] ability_by_lottery &lt;- ggplot(d) + aes(x=ability, fill = as.factor(lottery)) + geom_density(alpha=0.5) + labs(title=&#39;Ability by lottery&#39;) residuals_lottery &lt;- ggplot(d) + aes(x=residuals, fill=as.factor(lottery)) + geom_density(alpha=0.5) + labs(title = &#39;Residuals by lottery&#39;) residuals_ability &lt;- ggplot(d) + aes(x=residuals, fill=as.factor(ability&gt;10)) + geom_density(alpha=0.5) + labs(title = &#39;Residuals by ability&#39;) ability_by_lottery / residuals_lottery / residuals_ability Another way to think about this is in terms of how the predicted values are associated different features. Specifically, consider: Are the predicted values associated with having won the lottery? Are the predicted values associated with ability? predicted_lottery &lt;- ggplot(d) + aes(x=as.factor(lottery), y=schooling_hat) + geom_jitter() + labs(title=&#39;Predicted Values and Lottery Winning&#39;) predicted_ability &lt;- ggplot(d) + aes(x=ability, y=schooling_hat, color = as.factor(lottery)) + geom_jitter() + labs(title=&#39;Predicted Values and Ability&#39;) predicted_lottery / predicted_ability "],["regression-discontinuity.html", "10.6 Regression Discontinuity", " 10.6 Regression Discontinuity Regression discontinuity is a really clever idea, that when the data presents itself and the analysis is done correctly, provides a very compelling argument for having captured a causal effect. The key insight in the case of regression discontinuity is that we might not need something that is actually random in order to produce a credible treatment effect. All we need is treatment assignment mechanism that is not correlated with potential outcomes. And, the argument for regression discontinuity is that if you make a comparison set similar enough along a scoring variable, then it would be very hard for people on one-side or the other-side of an arbitrary point in the scoring variable to be different. 10.6.1 Why do RDD designs “work”? What part of the RDD is producing an unbiased causal estimate? Why is this part of he design/data generating process able to produce this unbiased causal estimate? What is a “forcing” variable? How do I identify where the cut-point in the forcing variable is located? 10.6.2 Just how common are opportunities for RDD Here’s a controversial point of view: Everything that we do as data scientists is to make low-dimensional representations of higher dimensional space. Let’s have a jam-session where the class and instructors take turns naming places where a RDD could be run. We’ll start with: Revolving line of credit – credit scoring models bring in disparate streams of information, produce a low-dimensional 0-800 (or something like that…) rating and provide revolving lines of credit to different parts of the distribution. Now you… 10.6.3 Working with Regression Discontinuity Designs Let’s look at see what is happening when we’re working with RDD designs. To start, let’s build some data. Read through each of the lines below, and note what is happening with the data being created. (Notice that we are chaining together the data.table after we create y0 and again after we create y1.) N &lt;- 1000 d &lt;- data.table(id=1:N) d[ , &#39;:=&#39;( tau = rnorm(.N, mean=0, sd=2), running = runif(.N, min=-2, max=2), y0 = runif(.N, min=-1, max=1)) ][ , y1 := y0 + tau ][ , Y := ifelse(running &gt; 0, y1, y0)] With the data created, let’s quickly look at what we are working with. Does the following plot seem to capture the idea of a treatment effect? If so, why? If not, why not and how would you propose to change the plot? ggplot(d) + aes(x=running, y=Y) + geom_point() + stat_smooth(method = &#39;lm&#39;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; d[ , plot(x=running, y=Y, pch = 19, col=berkeley_blue)] ## NULL d[running &lt; 0, lines(lowess(running,Y), lwd=10, col=california_gold)] ## NULL d[running &gt; 0, lines(lowess(running,Y), lwd=10, col=california_gold)] ## NULL This has been very fortunate data. There is no trend across the running variable, and things seem mostly linear on both sides. Naturally, the real world is not so tidy. 10.6.4 More realistic data d &lt;- data.table(id=1:N) d[ , &#39;:=&#39;( running = runif(n=.N, min=0, max=10), cov1 = rnorm(n=.N)) ][ , Y := running * 0.1 - 0.2 * cov1 + 1 * I(running &gt; 5) + rnorm(n=.N)] d[ , plot(x=running, y=Y, col=berkeley_blue, pch=19)] ## NULL Is it clear if there is, or is not an effect in this data simply by looking at it? What if you put a smoother through the data? d[ , plot(x=running, y=Y, col=berkeley_blue, pch=19)] ## NULL d[ , lines(lowess(running, Y), col=california_gold, lwd=10)] ## NULL What if you break that smoother at the policy point? d[running &lt; 5.2 &amp; running &gt; 4.8 , plot(x=running, y=Y, col=berkeley_blue, pch=19)] ## NULL d[running &lt; 5 , lines(lowess(running, Y), col=california_gold, lwd=10)] ## NULL d[running &gt; 5 , lines(lowess(running, Y), col=california_gold, lwd=10)] ## NULL 10.6.5 What about even more challenging data? d &lt;- data.frame(running = runif(1000, min = 0, max = 10), cov1 = rnorm(1000)) d$y &lt;- d$running * 0.1 - .2 * d$cov1 + 1 * I(d$running &gt; 5) + .4 * d$running * I(d$running &gt; 5) + rnorm(1000) plot(x = d$running, d$y, pch = 19, col = rgb(0,1,0, .4)) lines(lowess(d$running[d$running &lt; 5], d$y[d$running &lt; 5])) lines(lowess(d$running[d$running &gt; 5], d$y[d$running &gt; 5])) What model would you fit against this data? "],["problems-and-diagnostics.html", "Unit 11 Problems and Diagnostics ", " Unit 11 Problems and Diagnostics "],["learning-objectives-10.html", "11.1 Learning Objectives", " 11.1 Learning Objectives "],["attrition-mediation-and-generalizability.html", "Unit 12 Attrition, Mediation, and Generalizability", " Unit 12 Attrition, Mediation, and Generalizability The theme for this week, as we mention in the async, is that these are hard problems – in fact, each of these problems are so hard that we do not have an ability to place a clear, numerical answer on any of them. "],["learning-objectives-11.html", "12.1 Learning Objectives", " 12.1 Learning Objectives At the conclusion of this week, students will be able to Recognize attrition, distinguish the differences between attrition and compliance; design an experimental protocol to minimize the amount of attrition that is present in their data; and analyze an experiment that has experienced attrition to provide best-possible, defensible estimates of treatment. Reason about why one things causes another; reason about how this affects the ways that they design an experiment or treatment; but, also communicate why it is so difficult to produce clear evidence about why something has an effect. "],["why-doesnt-mediation-analysis-work.html", "12.2 Why doesn’t mediation analysis work?", " 12.2 Why doesn’t mediation analysis work? Here’s a classic case, that is actually very recent. Gaesser et al (2020) present subjects with a short text that describes a stranger in need, for example, someone who has fallen off a motorcycle on the freeway. Treatment Group members were asked to imagine helping the person who had fallen of the motorcycle. Control Group members were asked to critique the the writing style of the text that they read. Both Groups were shown the same text. Unsurprisingly, the authors found that the episodic simulation treatment increased individuals williness to help the stranger in need. But why? The authors suppose that there are three possible reasons why episodic simulation might work differently. It might work differently depending on how well someone can visualize the scene (scene vividness) measured by response to the question, “The imagined scene of helping in your mind was [1. not coherent … 7. coherent].” It might work differently depending on how well someone can visualize the person (person vividness) measured by response to the question, “Did you visualize the person in your mind?” [1. No, not at all … 7. vividly, as if currently there]. It enables empathetic thought (perspective taking) measured by response to the question, “Did you consider the other person’s thoughts and feelings?” [1. No, not at all … 7. Strongly considered.] Implicit mediation analysis works in the following way: \\[ \\begin{aligned} lm(M &amp;\\sim \\alpha_{1} + aX_{i} + \\epsilon_{1}) \\\\ lm(Y &amp;\\sim \\alpha_{2} + cX_{i} + \\epsilon_{2}) \\\\ lm(Y &amp;\\sim \\alpha_{3} + bM_{i} + c&#39;X_{i} + \\epsilon_{3}) \\end{aligned} \\] Where people talk about \\(c\\) as the “total effect” of \\(X\\) on \\(Y\\), and the “direct effect” of \\(X\\) on \\(Y\\) as the estimate that is reported in \\(c&#39;\\). Can you draw this system out in the way that we did in 203? Use circles to represent concepts that you’re measuring, and directed arrows to represent causal relationships between these concepts. plot(x=1,y=1, xlim = c(0, 1), ylim = c(0,1), type = &#39;n&#39;) Once you’ve written out these pathways, what could go wrong in this analysis? "],["endless-chain-of-why.html", "12.3 Endless Chain of Why?", " 12.3 Endless Chain of Why? Return back to the example that we talked about at the very first week of class, that living in a suburban environment causes a measurable increase in people’s BMI. In a five-minute breakout room, produce an enumerated list of theories about why living in the suburbs might increase someone’s BMI. The team that lists the greatest number possible causes gets a gold star. The team that lists the most hilarious possible cause also gets a gold star. "],["design-an-experiment-to-evaluate-these-possible-causes.html", "12.4 Design an experiment to evaluate these possible causes", " 12.4 Design an experiment to evaluate these possible causes Now that we’ve got the list of causes created, and discussed, let’s pick a smaller set of the possible causes, and have each group go back into their breakout room for five more minutes to specifically design an experiment that would produce evidence in support of (or in contrast to) their specific theory. Here’s the thing: in creating your test, you’re trying as best as possible to isolate one, and only one mechanism. So, an experiment that is able to change only a single mechanism is preferred to an experiment that tests who mechanisms at once. When we come back from this breakout, each team will spend three minutes presenting the design that they produced to test their theory, and the other groups will reason about whether there are other mechanisms that could be at play in producing differences in outcomes. "],["generalizability.html", "12.5 Generalizability", " 12.5 Generalizability Recall the Arizona towel example that we read in Field Experiments. It goes something like this, “There is a door hanger that goes into the bathroom of a Best Western that asks individuals to reuse their towels in an effort to lower environmental impact. There is a large effect in the first period of the study, but there is a smaller, and statistically insignificant effect in the second period of the study. Bates and Glennerster suggest four misguided approaches that might better be called, ways that other people think about generalizability, but the headings are rather misleading. Recast the headings into four more descriptive sentences instead. I’ll do the first for you: An effect learned in a particular context (or location) can never be informative of another location. Bates and Glenerster suggest a second four-item way to instead reason about generalizability. What are these four steps? Describe what each step means? Now, suppose that you’re the decision-maker who has to decide whether to run the experiment signs about towel re-use in Arizona (now for a third trial). How woud you use the four-step framework to evaluate whether to run another experiment? Throughout the async, David Broockman highlights the extreme difficulty in generating data that tests mechanisms. So, isn’t the Bates and Glennerster argument tantamount to saying, “Just think about this impossible thing that you’re never going to be able to measure?” Or, can you use their framework profitably to generalize to other contexts? "],["applications-of-experiments.html", "Unit 13 Applications of Experiments ", " Unit 13 Applications of Experiments "],["learning-objectives-12.html", "13.1 Learning Objectives", " 13.1 Learning Objectives "],["review-of-the-course.html", "Unit 14 Review of the Course ", " Unit 14 Review of the Course "],["learning-objectives-13.html", "14.1 Learning Objectives", " 14.1 Learning Objectives David Reiley, D. Alex Hughes, David Broockman, Micah Gell-Redman, Scott Guenther, David Wheeler, Josue Martinze↩︎ This is a footnote, rendered into an html document.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
